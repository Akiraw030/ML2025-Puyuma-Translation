{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T13:22:34.281173Z",
     "iopub.status.busy": "2025-06-11T13:22:34.280957Z",
     "iopub.status.idle": "2025-06-11T13:22:34.512976Z",
     "shell.execute_reply": "2025-06-11T13:22:34.512035Z",
     "shell.execute_reply.started": "2025-06-11T13:22:34.281148Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 11 13:22:34 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   47C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   53C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T13:22:36.857362Z",
     "iopub.status.busy": "2025-06-11T13:22:36.856681Z",
     "iopub.status.idle": "2025-06-11T13:23:57.083662Z",
     "shell.execute_reply": "2025-06-11T13:23:57.082765Z",
     "shell.execute_reply.started": "2025-06-11T13:22:36.857340Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Package download\n",
    "\n",
    "!pip install sentencepiece -q\n",
    "!pip install transformers -q\n",
    "!pip install datasets -q\n",
    "!pip install peft -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T12:06:21.982842Z",
     "iopub.status.busy": "2025-06-10T12:06:21.982616Z",
     "iopub.status.idle": "2025-06-10T12:07:14.333970Z",
     "shell.execute_reply": "2025-06-10T12:07:14.333138Z",
     "shell.execute_reply.started": "2025-06-10T12:06:21.982819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de25822030e4a1da35373d78fee44b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7e66c8ab8247c0859586cb4d3c3c9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)6cea38b9e3d5efcdcb9c251d6b40538e1aab555a:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0c35a9b2674cca8df5ca641b7b7075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)b3c438311629547285129b0b81dadabd01bca665:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0824050afd2c4f3a92b0dfdfc5cd7cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8795b32de673418a8c454dce285d7cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 12:06:43.252933: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749557203.649316      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749557203.782481      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ef712928974197b6813473a40f2e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)1ecdf1e485509035f6b51dfe84f1ada83eefcc42:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93c8550796e4ee29ca5476b2668478e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0332b840fb0d49cdb105868335a8fadf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Nllb loading\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "# model_name = \"facebook/nllb-200-3.3B\" # Larger model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.src_lang = \"zho_Hant\"\n",
    "tokenizer.tgt_lang = \"tgl_Latn\"\n",
    "# zho_Hant for Chinese traditional\n",
    "# eng_Latn for English\n",
    "# tgl_Latn for Puyuma (Use existing language tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T13:25:27.431776Z",
     "iopub.status.busy": "2025-06-11T13:25:27.431218Z",
     "iopub.status.idle": "2025-06-11T13:25:27.760134Z",
     "shell.execute_reply": "2025-06-11T13:25:27.759275Z",
     "shell.execute_reply.started": "2025-06-11T13:25:27.431751Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load data into dataframes\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "lexicon = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/lexicon_no_en.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\n",
    "lexicon.columns = ['pyu', 'zho']\n",
    "\n",
    "lexicon_en = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/lexicon.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\n",
    "lexicon_en.columns = ['pyu', 'eng', 'zho']\n",
    "\n",
    "sentences = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/sentences_no_en.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\n",
    "sentences.columns = ['pyu', 'zho']\n",
    "\n",
    "sentences_en = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/sentences.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\n",
    "sentences_en.columns = ['pyu', 'eng', 'zho']\n",
    "\n",
    "#lexicon.sample(5)\n",
    "#lexicon_en.sample(10)\n",
    "#sentences.sample(5)\n",
    "#sentences_en.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T12:07:14.426945Z",
     "iopub.status.busy": "2025-06-10T12:07:14.426649Z",
     "iopub.status.idle": "2025-06-10T12:07:22.933167Z",
     "shell.execute_reply": "2025-06-10T12:07:22.932286Z",
     "shell.execute_reply.started": "2025-06-10T12:07:14.426920Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toks per word of lexicon:\n",
      "2.2840203274985886\n",
      "2.3545659526493803\n",
      "toks per word of lexicon_en:\n",
      "1.738603297769156\n",
      "2.2698515171078117\n",
      "toks per word of sentences:\n",
      "1.5806060606060606\n",
      "3.925910765452312\n",
      "toks per word of sentences_en:\n",
      "1.5050038491147038\n",
      "3.165337423312884\n",
      "total unk in lexicon zho:\n",
      "97\n",
      "total unk in lexicon pyu:\n",
      "0\n",
      "total unk in lexicon_en zho:\n",
      "203\n",
      "total unk in lexicon_en pyu:\n",
      "0\n",
      "total unk in sentences zho:\n",
      "223\n",
      "total unk in sentences pyu:\n",
      "36\n",
      "total unk in sentences_en zho:\n",
      "142\n",
      "total unk in sentences pyu:\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# Testing the performances of original tokenization\n",
    "\n",
    "import re\n",
    "\n",
    "def word_tokenize(text):\n",
    "    \n",
    "    return re.findall('(\\w+|[^\\w\\s])', text)\n",
    "\n",
    "def df_tokenize(df):\n",
    "    df['pyu_toks'] = df.pyu.apply(tokenizer.tokenize)\n",
    "    df['zho_toks'] = df.zho.apply(tokenizer.tokenize)\n",
    "    df['pyu_words'] = df.pyu.apply(word_tokenize)\n",
    "    df['zho_words'] = df.zho.apply(word_tokenize)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def cal_tokperword(df):\n",
    "\n",
    "    stats = df[['pyu_toks', 'zho_toks', 'pyu_words', 'zho_words']].map(len).describe()\n",
    "    print(stats.pyu_toks['mean'] / stats.pyu_words['mean'])\n",
    "    print(stats.zho_toks['mean'] / stats.zho_words['mean'])\n",
    "\n",
    "    return stats\n",
    "\n",
    "def check_unk(df, column):\n",
    "\n",
    "    texts_with_unk = [\n",
    "        text for text in df[column]\n",
    "        if tokenizer.unk_token_id in tokenizer(text).input_ids\n",
    "    ]\n",
    "    print(len(texts_with_unk))\n",
    "\n",
    "lexicon = df_tokenize(lexicon)\n",
    "lexicon_en = df_tokenize(lexicon_en)\n",
    "sentences = df_tokenize(sentences)\n",
    "sentences_en = df_tokenize(sentences_en)\n",
    "\n",
    "print(\"toks per word of lexicon:\")\n",
    "stats_lexicon = cal_tokperword(lexicon)\n",
    "print(\"toks per word of lexicon_en:\")\n",
    "stats_lexicon = cal_tokperword(lexicon_en)\n",
    "print(\"toks per word of sentences:\")\n",
    "stats_sentences = cal_tokperword(sentences)\n",
    "print(\"toks per word of sentences_en:\")\n",
    "stats_sentences = cal_tokperword(sentences_en)\n",
    "\n",
    "print(\"total unk in lexicon zho:\")\n",
    "check_unk(lexicon, \"zho\")\n",
    "print(\"total unk in lexicon pyu:\")\n",
    "check_unk(lexicon, \"pyu\")\n",
    "print(\"total unk in lexicon_en zho:\")\n",
    "check_unk(lexicon_en, \"zho\")\n",
    "print(\"total unk in lexicon_en pyu:\")\n",
    "check_unk(lexicon_en, \"pyu\")\n",
    "print(\"total unk in sentences zho:\")\n",
    "check_unk(sentences, \"zho\")\n",
    "print(\"total unk in sentences pyu:\")\n",
    "check_unk(sentences, \"pyu\")\n",
    "print(\"total unk in sentences_en zho:\")\n",
    "check_unk(sentences_en, \"zho\")\n",
    "print(\"total unk in sentences pyu:\")\n",
    "check_unk(sentences_en, \"pyu\")\n",
    "\n",
    "#show datas\n",
    "#lexicon.sample(10)\n",
    "#sentences.sample(10)\n",
    "#stats_lexicon\n",
    "#stats_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T12:07:22.934391Z",
     "iopub.status.busy": "2025-06-10T12:07:22.934027Z",
     "iopub.status.idle": "2025-06-10T12:07:31.042169Z",
     "shell.execute_reply": "2025-06-10T12:07:31.041208Z",
     "shell.execute_reply.started": "2025-06-10T12:07:22.934366Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638702d4978e4ff2bbace1d869be82b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8770 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "需要強制包含的單字元: ’“”仟傻儼兇兜冀凳刮剁剝劈勻厲叢叮吱吵吼咀咕咱哇哎唉唬唷啃啄啥喂喔嗚嗯嘻噁噎嚀嚏嚕嚥嚨嚼囉囑坍垮墮墾壑夾妝妳姑姨婿媳嫂嬸宋寞屁屎屹岔峨峭峽嶺嶼巍巒廚廨彎徊徘怔怡惱懶扁扛扯抖拂拇拌拚捻掀掐揉揍揹搓搔搗搥摺撐撥撩撬擠擲攀攜攤攪攬敞晃晉晾暈杓杵柑柚桐梳椒椽楊榔槌樑樸橘橡檜檳櫻殷氓氾汙洶涎涕淒湃湊湛溉溼滷漱漾漿潑澀澆澈澎濁濕濤瀉瀑灑灶炒烘烹煞煥熬燉燙燦燻爐犁猴琉瓢甕甩畝疊疤痠痰癢皂盈盪眶睏睜瞄瞌瞧瞰矚砌碩碴碾磚礱稻窩竄竊竿笆筌筍筷箏箕篩篳簍簷籃籬粥粿糠糬糯糰緻縷繡繩羌羸耆耍耙聆脖腋膛膿臀臼舀舅舔舖芋芙苣苧茅莓菇菸萵葵蓆蔔蔗蔥蕃蕉蕎蕗蕨薑薯藷蘿蚓蚤蚯蚱蛀蛙蜓蜢蜻蝦蝨蝴蝸螂螃螞蟑蟬蟹蟻蠅裙諧諱謠謾豎豹贛趴跛踢蹂蹦蹲躪辣迴逛遐邈邵鄒酗醃醬釀鈔鉤鋤鋸錘錶鍊鍬鎚鏟鏽鐮鐺鑰閒閩闆闡闢阱陡雉霜鞦韆颱颳飧餚餵饋駝驟髒鬍鬚鬧魄鳳鴨鴿鵝鹹鹽麴齒龜\n"
     ]
    }
   ],
   "source": [
    "# Training tokenizer for missing tokens\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "from collections import Counter\n",
    "import sentencepiece as spm\n",
    "from datasets import load_dataset\n",
    "\n",
    "all_texts = lexicon['zho'].dropna().tolist() + sentences['zho'].dropna().tolist() + lexicon_en['zho'].dropna().tolist() + sentences_en['zho'].dropna().tolist() + lexicon['pyu'].dropna().tolist() + sentences['pyu'].dropna().tolist() + lexicon_en['pyu'].dropna().tolist() + sentences_en['pyu'].dropna().tolist()\n",
    "\n",
    "all_texts_file = 'all_texts_plain.txt'\n",
    "with open(all_texts_file, 'w', encoding='utf-8') as f:\n",
    "    for text in all_texts:\n",
    "        print(text, file=f)\n",
    "\n",
    "required_chars = set()\n",
    "\n",
    "for text in tqdm(all_texts):\n",
    "    for char in text:\n",
    "        tokens = tokenizer.tokenize(char)\n",
    "        if tokens == ['▁', '<unk>']:\n",
    "            required_chars.add(char)\n",
    "\n",
    "required_chars_str = \"\".join(sorted(list(required_chars)))\n",
    "print(f\"需要強制包含的單字元: {required_chars_str}\")\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=all_texts_file,\n",
    "    model_prefix='spm_new',\n",
    "    vocab_size=5800,\n",
    "    character_coverage=1,\n",
    "    num_threads=16,\n",
    "    train_extremely_large_corpus=False,\n",
    "    add_dummy_prefix=False,\n",
    "    max_sentencepiece_length=128,\n",
    "    max_sentence_length=4192 * 4,\n",
    "    pad_id=0,\n",
    "    eos_id=1,\n",
    "    unk_id=2,\n",
    "    bos_id=-1,\n",
    "    required_chars=required_chars_str,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T12:07:31.044077Z",
     "iopub.status.busy": "2025-06-10T12:07:31.043323Z",
     "iopub.status.idle": "2025-06-10T12:07:37.986497Z",
     "shell.execute_reply": "2025-06-10T12:07:37.985867Z",
     "shell.execute_reply.started": "2025-06-10T12:07:31.044034Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256204 6004\n"
     ]
    }
   ],
   "source": [
    "# Add trained tokens to tokenizer and model\n",
    "\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n",
    "from transformers import NllbTokenizer\n",
    "\n",
    "model_name = 'facebook/nllb-200-distilled-600M'\n",
    "tokenizer_nllb = NllbTokenizer.from_pretrained(model_name)\n",
    "\n",
    "sp_trained = spm.SentencePieceProcessor(model_file='spm_new.model')\n",
    "added_spm = sp_pb2_model.ModelProto()\n",
    "added_spm.ParseFromString(sp_trained.serialized_model_proto())\n",
    "old_spm_nllb = sp_pb2_model.ModelProto()\n",
    "old_spm_nllb.ParseFromString(tokenizer_nllb.sp_model.serialized_model_proto())\n",
    "\n",
    "nllb_tokens_set = {p.piece for p in old_spm_nllb.pieces}\n",
    "prev_min_score = old_spm_nllb.pieces[-1].score\n",
    "for p in added_spm.pieces:\n",
    "    piece = p.piece\n",
    "    if p.type != 1:\n",
    "        continue\n",
    "    if piece not in nllb_tokens_set:\n",
    "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
    "        new_p.piece = piece\n",
    "        new_p.score = p.score + prev_min_score\n",
    "        old_spm_nllb.pieces.append(new_p)\n",
    "\n",
    "NEW_SPM_NAME = 'spm_nllb_extended_268k.model'\n",
    "with open(NEW_SPM_NAME, 'wb') as f:\n",
    "    f.write(old_spm_nllb.SerializeToString())\n",
    "\n",
    "tokenizer = NllbTokenizer.from_pretrained(model_name, vocab_file='spm_new.model')\n",
    "print(len(tokenizer_nllb), len(tokenizer))\n",
    "added_vocab = set(tokenizer.get_vocab()).difference(set(tokenizer_nllb.get_vocab()))\n",
    "#print(added_vocab)(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T12:07:37.987550Z",
     "iopub.status.busy": "2025-06-10T12:07:37.987263Z",
     "iopub.status.idle": "2025-06-10T12:07:39.283558Z",
     "shell.execute_reply": "2025-06-10T12:07:39.282928Z",
     "shell.execute_reply.started": "2025-06-10T12:07:37.987525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers.optimization import Adafactor\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "model.cuda();\n",
    "optimizer = Adafactor(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    scale_parameter=False,\n",
    "    relative_step=False,\n",
    "    lr=1e-4,\n",
    "    clip_threshold=1.0,\n",
    "    weight_decay=1e-3,\n",
    ")\n",
    "scheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T12:07:39.284556Z",
     "iopub.status.busy": "2025-06-10T12:07:39.284303Z",
     "iopub.status.idle": "2025-06-10T12:07:39.294548Z",
     "shell.execute_reply": "2025-06-10T12:07:39.293791Z",
     "shell.execute_reply.started": "2025-06-10T12:07:39.284529Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['maratrang'], ['懶惰'], 'tgl_Latn', 'zho_Hant')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "LANGS = [('zho', 'zho_Hant'), ('pyu', 'tgl_Latn')]\n",
    "\n",
    "dfs = [lexicon, sentences, lexicon_en, sentences_en]\n",
    "df_train = pd.concat([df[['pyu', 'zho']] for df in dfs], ignore_index=True)\n",
    "\n",
    "def get_batch_pairs(batch_size, data=df_train):\n",
    "    (l1, long1), (l2, long2) = random.sample(LANGS, 2)\n",
    "    xx, yy = [], []\n",
    "    for _ in range(batch_size):\n",
    "        item = data.iloc[random.randint(0, len(data)-1)]\n",
    "        xx.append(item[l1])\n",
    "        yy.append(item[l2])\n",
    "    return xx, yy, long1, long2\n",
    "\n",
    "print(get_batch_pairs(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T12:07:39.296995Z",
     "iopub.status.busy": "2025-06-10T12:07:39.296771Z",
     "iopub.status.idle": "2025-06-10T12:07:39.318919Z",
     "shell.execute_reply": "2025-06-10T12:07:39.318383Z",
     "shell.execute_reply.started": "2025-06-10T12:07:39.296978Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16  # 32 already doesn't fit well to 15GB of GPU memory\n",
    "max_length = 128  # token sequences will be truncated\n",
    "training_steps = 50000  # Usually, I set a large number of steps,\n",
    "# and then just interrupt the training manually\n",
    "losses = []  # with this list, I do very simple tracking of average loss\n",
    "MODEL_SAVE_PATH = '/kaggle/working/nllb_extended'  # on my Google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-10T15:43:10.432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "def cleanup():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model.train()\n",
    "x, y, loss = None, None, None\n",
    "cleanup()\n",
    "\n",
    "tq = trange(len(losses), training_steps)\n",
    "for i in tq:\n",
    "    xx, yy, lang1, lang2 = get_batch_pairs(batch_size)\n",
    "    try:\n",
    "        tokenizer.src_lang = lang1\n",
    "        x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n",
    "        tokenizer.src_lang = lang2\n",
    "        y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n",
    "        # -100 is a magic value ignored in the loss function\n",
    "        # because we don't want the model to learn to predict padding ids\n",
    "        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n",
    "\n",
    "        loss = model(**x, labels=y.input_ids).loss\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scheduler.step()\n",
    "\n",
    "    except RuntimeError as e:  # usually, it is out-of-memory\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        x, y, loss = None, None, None\n",
    "        cleanup()\n",
    "        print('error', max(len(s) for s in xx + yy), e)\n",
    "        continue\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        # each 1000 steps, I report average loss at these steps\n",
    "        print(i, np.mean(losses[-1000:]))\n",
    "\n",
    "    if i % 1000 == 0 and i > 0:\n",
    "        model.save_pretrained(MODEL_SAVE_PATH)\n",
    "        tokenizer.save_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T09:24:30.129711Z",
     "iopub.status.busy": "2025-06-11T09:24:30.128985Z",
     "iopub.status.idle": "2025-06-11T09:24:30.198055Z",
     "shell.execute_reply": "2025-06-11T09:24:30.197510Z",
     "shell.execute_reply.started": "2025-06-11T09:24:30.129689Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import NllbTokenizer, AutoModelForSeq2SeqLM\n",
    "model_dir = '/kaggle/input/nllb-extended/other/45000steps/1/results/nllb_extended'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir, local_files_only=True).cuda()\n",
    "tokenizer = NllbTokenizer.from_pretrained(model_dir, vocab_file='/kaggle/input/nllb-extended/other/45000steps/1/results/spm_new.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T09:29:14.531700Z",
     "iopub.status.busy": "2025-06-11T09:29:14.531400Z",
     "iopub.status.idle": "2025-06-11T09:29:14.689735Z",
     "shell.execute_reply": "2025-06-11T09:29:14.689087Z",
     "shell.execute_reply.started": "2025-06-11T09:29:14.531679Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'azi ku miinurung za paisu.\"]\n"
     ]
    }
   ],
   "source": [
    "def translate(\n",
    "    text, src_lang='zho_Hant', tgt_lang='tgl_Latn', \n",
    "    a=32, b=3, max_input_length=1024, num_beams=1, **kwargs\n",
    "):\n",
    "    \"\"\"Turn a text or a list of texts into a list of translations\"\"\"\n",
    "    tokenizer.src_lang = src_lang\n",
    "    tokenizer.tgt_lang = tgt_lang\n",
    "    inputs = tokenizer(\n",
    "        text, return_tensors='pt', padding=True, truncation=True, \n",
    "        max_length=max_input_length\n",
    "    )\n",
    "    model.eval() # turn off training mode\n",
    "    result = model.generate(\n",
    "        **inputs.to(model.device),\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n",
    "        num_beams=num_beams, **kwargs\n",
    "    )\n",
    "    return tokenizer.batch_decode(result, skip_special_tokens=True)\n",
    "\n",
    "t = '我也沒帶錢耶!'\n",
    "print(translate(t, 'zho_Hant', 'tgl_Latn'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T13:35:15.372294Z",
     "iopub.status.busy": "2025-06-11T13:35:15.371620Z",
     "iopub.status.idle": "2025-06-11T13:35:23.584000Z",
     "shell.execute_reply": "2025-06-11T13:35:23.583112Z",
     "shell.execute_reply.started": "2025-06-11T13:35:15.372274Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>aaydan ziya na rahan naulid i zekalr zi natrep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>a treme'utran mu, mutrungutrungulr kana 'azi m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>a menadanadam muwaarak kanizu na suwan mu, ulr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>a puresaring, ki物'urian到valenan, kitrepa kana ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>, maranger ku matemuy za semangalan, maranger ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                             answer\n",
       "0   1  aaydan ziya na rahan naulid i zekalr zi natrep...\n",
       "1   2  a treme'utran mu, mutrungutrungulr kana 'azi m...\n",
       "2   3  a menadanadam muwaarak kanizu na suwan mu, ulr...\n",
       "3   4  a puresaring, ki物'urian到valenan, kitrepa kana ...\n",
       "4   5  , maranger ku matemuy za semangalan, maranger ..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_dir = \"/kaggle/input/nllb-extended/other/45000steps/1/results/nllb_extended\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir, local_files_only=True).to(\"cuda\")\n",
    "\n",
    "def translate(\n",
    "    text, src_lang='zho_Hant', tgt_lang='tgl_Latn', \n",
    "    a=32, b=3, max_input_length=1024, num_beams=1, **kwargs\n",
    "):\n",
    "    tokenizer.src_lang = src_lang\n",
    "    tokenizer.tgt_lang = tgt_lang\n",
    "    inputs = tokenizer(\n",
    "        text, return_tensors='pt', padding=True, truncation=True, \n",
    "        max_length=max_input_length\n",
    "    ).to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    result = model.generate(\n",
    "        **inputs,\n",
    "        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n",
    "        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n",
    "        num_beams=num_beams, **kwargs\n",
    "    )\n",
    "    decoded = tokenizer.batch_decode(result, skip_special_tokens=True)\n",
    "\n",
    "    # 如果解碼結果是空字串，就用 'ERROR' 代替\n",
    "    decoded = [t if t.strip() != \"\" else \"ERROR\" for t in decoded]\n",
    "\n",
    "    return decoded\n",
    "\n",
    "to_pyu = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/zh_to_pyu_test.csv', header=None)[0]\n",
    "to_zho = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/pyu_to_zh_test.csv', header=None)[0]\n",
    "\n",
    "translated_pyu = translate(to_pyu.tolist(), src_lang='zho_Hant', tgt_lang='pyu_Latn')\n",
    "translated_zho = translate(to_zho.tolist(), src_lang='pyu_Latn', tgt_lang='zho_Hant')\n",
    "\n",
    "final = pd.DataFrame({\n",
    "    \"ID\": range(1, len(translated_pyu) + len(translated_zho) + 1),\n",
    "    \"answer\": translated_pyu + translated_zho\n",
    "})\n",
    "final['answer'] = final['answer'].fillna('ERROR')\n",
    "final.to_csv(\"submission.csv\", index=False, encoding='utf-8')\n",
    "final.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7304401,
     "sourceId": 11640807,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 373359,
     "modelInstanceId": 352091,
     "sourceId": 431918,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
