{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12146153,"sourceType":"datasetVersion","datasetId":7649884},{"sourceId":12184021,"sourceType":"datasetVersion","datasetId":7674078},{"sourceId":431918,"sourceType":"modelInstanceVersion","modelInstanceId":352091,"modelId":373359}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# GPU check\n\n!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:55:36.610042Z","iopub.execute_input":"2025-06-16T15:55:36.610644Z","iopub.status.idle":"2025-06-16T15:55:36.832939Z","shell.execute_reply.started":"2025-06-16T15:55:36.610617Z","shell.execute_reply":"2025-06-16T15:55:36.832025Z"}},"outputs":[{"name":"stdout","text":"Mon Jun 16 15:55:36 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   42C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   45C    P8              9W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Package download\n\n!pip install sentencepiece transformers datasets -q\n\n!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122 -q\n\n!wget https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K.gguf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:55:38.165717Z","iopub.execute_input":"2025-06-16T15:55:38.166033Z","iopub.status.idle":"2025-06-16T15:56:09.938420Z","shell.execute_reply.started":"2025-06-16T15:55:38.166008Z","shell.execute_reply":"2025-06-16T15:56:09.937626Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m224.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h--2025-06-16 15:56:01--  https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K.gguf\nResolving huggingface.co (huggingface.co)... 18.160.143.99, 18.160.143.32, 18.160.143.76, ...\nConnecting to huggingface.co (huggingface.co)|18.160.143.99|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cas-bridge.xethub.hf.co/xet-bridge-us/66f457f561dbc064f4a6413f/da1f5f07d4a337c67bfa4bc7ed0a241a4fe77883ba5924357b8c2e1db5ec797d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250616%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250616T155601Z&X-Amz-Expires=3600&X-Amz-Signature=30615ad2377cf969021e35e8cf15052497bb2fbc2b45e8bf1edc9c2b1feacf0c&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-3B-Instruct-Q6_K.gguf%3B+filename%3D%22Llama-3.2-3B-Instruct-Q6_K.gguf%22%3B&x-id=GetObject&Expires=1750092961&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDA5Mjk2MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NmY0NTdmNTYxZGJjMDY0ZjRhNjQxM2YvZGExZjVmMDdkNGEzMzdjNjdiZmE0YmM3ZWQwYTI0MWE0ZmU3Nzg4M2JhNTkyNDM1N2I4YzJlMWRiNWVjNzk3ZCoifV19&Signature=bNtH-CNW%7EadSO1QVAFQoWCmqtqn027vEkLlyuyvJJ9a3UcbA5v7TU9T1Sb7O8rrIlg2OA%7Eh18BOtSOVEcewuCLLFkc8iu%7EYYb8KzXFt9%7EkWSYKmHDU6F7lMEFWJiHQSBB-RJ3CBBi1BoFVq99yho%7E12O3z4OBSp61nq%7EvfkOSmawltVGCa8ReuLitqgsbstiETrpJRA5C-otKELHFUaIdBYfH1o-ZDScheKZTXEG6XY0vJnJyLnKnFe-qmjXY1MJiRY%7EJy%7EngQUHmRy68EioUqk0s2kO4Twc0wL6XQpyIl6RyVT7l9q7d-nMMXHcEwdiniIG4bXsf6WTL6um7DQl6g__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n--2025-06-16 15:56:01--  https://cas-bridge.xethub.hf.co/xet-bridge-us/66f457f561dbc064f4a6413f/da1f5f07d4a337c67bfa4bc7ed0a241a4fe77883ba5924357b8c2e1db5ec797d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250616%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250616T155601Z&X-Amz-Expires=3600&X-Amz-Signature=30615ad2377cf969021e35e8cf15052497bb2fbc2b45e8bf1edc9c2b1feacf0c&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-3B-Instruct-Q6_K.gguf%3B+filename%3D%22Llama-3.2-3B-Instruct-Q6_K.gguf%22%3B&x-id=GetObject&Expires=1750092961&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MDA5Mjk2MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NmY0NTdmNTYxZGJjMDY0ZjRhNjQxM2YvZGExZjVmMDdkNGEzMzdjNjdiZmE0YmM3ZWQwYTI0MWE0ZmU3Nzg4M2JhNTkyNDM1N2I4YzJlMWRiNWVjNzk3ZCoifV19&Signature=bNtH-CNW%7EadSO1QVAFQoWCmqtqn027vEkLlyuyvJJ9a3UcbA5v7TU9T1Sb7O8rrIlg2OA%7Eh18BOtSOVEcewuCLLFkc8iu%7EYYb8KzXFt9%7EkWSYKmHDU6F7lMEFWJiHQSBB-RJ3CBBi1BoFVq99yho%7E12O3z4OBSp61nq%7EvfkOSmawltVGCa8ReuLitqgsbstiETrpJRA5C-otKELHFUaIdBYfH1o-ZDScheKZTXEG6XY0vJnJyLnKnFe-qmjXY1MJiRY%7EJy%7EngQUHmRy68EioUqk0s2kO4Twc0wL6XQpyIl6RyVT7l9q7d-nMMXHcEwdiniIG4bXsf6WTL6um7DQl6g__&Key-Pair-Id=K2L8F4GPSG1IFC\nResolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 108.156.201.104, 108.156.201.113, 108.156.201.5, ...\nConnecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|108.156.201.104|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2643853856 (2.5G)\nSaving to: ‘Llama-3.2-3B-Instruct-Q6_K.gguf’\n\nLlama-3.2-3B-Instru 100%[===================>]   2.46G   291MB/s    in 8.3s    \n\n2025-06-16 15:56:09 (302 MB/s) - ‘Llama-3.2-3B-Instruct-Q6_K.gguf’ saved [2643853856/2643853856]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Package import\n\nimport gc\nimport random\nimport asyncio\nfrom typing import Dict, List, Tuple\nfrom tqdm.auto import tqdm, trange\nfrom collections import Counter\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nfrom datasets import load_dataset\n\nimport re\nimport jieba\n\nimport torch\nfrom transformers import NllbTokenizer, AutoModelForSeq2SeqLM, get_constant_schedule_with_warmup\nfrom transformers.optimization import Adafactor\nimport sentencepiece as spm\nfrom sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n\nfrom llama_cpp import Llama","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:56:26.733611Z","iopub.execute_input":"2025-06-16T15:56:26.733923Z","iopub.status.idle":"2025-06-16T15:56:26.738847Z","shell.execute_reply.started":"2025-06-16T15:56:26.733906Z","shell.execute_reply":"2025-06-16T15:56:26.738137Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load datasets into dataframes\n\nlexicon = pd.read_csv('/kaggle/input/dataset/lexicon_no_en.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nlexicon.columns = ['pyu', 'zho']\n\nlexicon_en = pd.read_csv('/kaggle/input/dataset/lexicon.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nlexicon_en.columns = ['pyu', 'eng', 'zho']\n\nsentences = pd.read_csv('/kaggle/input/dataset/sentences_no_en.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nsentences.columns = ['pyu', 'zho']\n\nsentences_en = pd.read_csv('/kaggle/input/dataset/sentences.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nsentences_en.columns = ['pyu', 'eng', 'zho']\n\nto_pyu = pd.read_csv('/kaggle/input/dataset/zh_to_pyu_test.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nto_pyu.columns = ['zho']\n\nto_zho = pd.read_csv('/kaggle/input/dataset/pyu_to_zh_test.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nto_zho.columns = ['pyu']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:56:26.740125Z","iopub.execute_input":"2025-06-16T15:56:26.740354Z","iopub.status.idle":"2025-06-16T15:56:26.853723Z","shell.execute_reply.started":"2025-06-16T15:56:26.740328Z","shell.execute_reply":"2025-06-16T15:56:26.853172Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Part1 Examine and Training Tokenizer","metadata":{}},{"cell_type":"code","source":"# Nllb tokenizer loading\n\nmodel_name = \"facebook/nllb-200-distilled-600M\"\n\ntokenizer = NllbTokenizer.from_pretrained(model_name)\n\ntokenizer.src_lang = \"zho_Hant\" # zho_Hant for Chinese traditional\ntokenizer.tgt_lang = \"tgl_Latn\" # tgl_Latn for Puyuma (Use existing language tag, originally for Tagalog)\n                                # eng_Latn for English","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:56:26.854262Z","iopub.execute_input":"2025-06-16T15:56:26.854450Z","iopub.status.idle":"2025-06-16T15:56:29.270711Z","shell.execute_reply.started":"2025-06-16T15:56:26.854436Z","shell.execute_reply":"2025-06-16T15:56:29.270101Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c1edd720977452ab13a40d25f00fe6d"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"(…)6cea38b9e3d5efcdcb9c251d6b40538e1aab555a:   0%|          | 0.00/4.85M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c3cc092d17c4bf4a7eb87917beff860"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/3.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f357f0e63e324ed88d1939b621979bdd"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"(…)b3c438311629547285129b0b81dadabd01bca665:   0%|          | 0.00/17.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc76f18eb08b47a69d48c7533288f0b8"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# Testing the performances of original tokenizer\n\n_WORD_RE = re.compile(r\"(\\w+|[^\\w\\s])\")\ndef word_tokenize(text: str) -> List[str]: # word tokenization for puyuma (jieba package for zho)\n    return _WORD_RE.findall(text)\n\ndef tokenize_dataframe(\n    df: pd.DataFrame,\n    tokenizer: tokenizer,\n    src_col: str = \"zho\",\n    tgt_col: str = \"pyu\",\n) -> pd.DataFrame:\n    \n    df = df.copy()\n\n    df[f\"{src_col}_toks\"] = df[src_col].apply(tokenizer.tokenize)\n    df[f\"{tgt_col}_toks\"] = df[tgt_col].apply(tokenizer.tokenize)\n\n    df[f\"{src_col}_words\"] = df[src_col].apply(lambda x: list(jieba.cut(x)))\n    df[f\"{tgt_col}_words\"] = df[tgt_col].apply(word_tokenize)\n\n    return df\n\n\ndef _mean_tokens_per_word(\n    tok_col: pd.Series, word_col: pd.Series\n) -> float:\n\n    return tok_col.map(len).mean() / word_col.map(len).mean()\n\n\ndef _count_unk(\n    texts: pd.Series, tokenizer: tokenizer\n) -> int:\n\n    return sum(tokenizer.unk_token_id in tokenizer(t).input_ids for t in texts)\n\n\ndef analyze_dataset(\n    df: pd.DataFrame,\n    name: str,\n    tokenizer: tokenizer,\n    src_col: str = \"zho\",\n    tgt_col: str = \"pyu\",\n) -> Dict[str, float]:\n    \n    report = {\n        \"dataset\": name,\n        \"mean_token_per_word_src\": _mean_tokens_per_word(\n            df[f\"{src_col}_toks\"], df[f\"{src_col}_words\"]\n        ),\n        \"mean_token_per_word_tgt\": _mean_tokens_per_word(\n            df[f\"{tgt_col}_toks\"], df[f\"{tgt_col}_words\"]\n        ),\n        \"num_sentence_with_unk_src\": _count_unk(df[src_col], tokenizer),\n        \"num_sentence_with_unk_tgt\": _count_unk(df[tgt_col], tokenizer),\n    }\n    return report\n\n\ndef run_all_analyses(\n    datasets: Dict[str, pd.DataFrame],\n    tokenizer: tokenizer,\n    src_col: str = \"zho\",\n    tgt_col: str = \"pyu\",\n) -> pd.DataFrame:\n    \n    reports: List[Dict[str, float]] = []\n\n    for name, raw_df in datasets.items():\n        df_tok = tokenize_dataframe(\n            raw_df, tokenizer, src_col=src_col, tgt_col=tgt_col\n        )\n        reports.append(\n            analyze_dataset(\n                df_tok, name, tokenizer, src_col=src_col, tgt_col=tgt_col\n            )\n        )\n\n    return pd.DataFrame(reports).set_index(\"dataset\")\n\n\ndatasets = {\n    \"lexicon\": lexicon,\n    \"lexicon_en\": lexicon_en,\n    \"sentences\": sentences,\n    \"sentences_en\": sentences_en,\n}\n\nsummary_df = run_all_analyses(datasets, tokenizer)\nprint(summary_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:56:29.271460Z","iopub.execute_input":"2025-06-16T15:56:29.271644Z","iopub.status.idle":"2025-06-16T15:56:31.824402Z","shell.execute_reply.started":"2025-06-16T15:56:29.271630Z","shell.execute_reply":"2025-06-16T15:56:31.823537Z"}},"outputs":[{"name":"stderr","text":"Building prefix dict from the default dictionary ...\nDumping model to file cache /tmp/jieba.cache\nLoading model cost 0.651 seconds.\nPrefix dict has been built successfully.\n","output_type":"stream"},{"name":"stdout","text":"              mean_token_per_word_src  mean_token_per_word_tgt  \\\ndataset                                                          \nlexicon                      1.819939                 2.284020   \nlexicon_en                   1.881565                 1.738603   \nsentences                    1.328140                 1.580606   \nsentences_en                 1.249425                 1.505004   \n\n              num_sentence_with_unk_src  num_sentence_with_unk_tgt  \ndataset                                                             \nlexicon                              97                          0  \nlexicon_en                          203                          0  \nsentences                           223                         36  \nsentences_en                        142                         14  \n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Training new tokenizer for missing tokens\n\ndatasets = [lexicon, sentences, lexicon_en, sentences_en, to_pyu, to_zho]\nlanguages = ['zho', 'pyu']\nall_texts = []\n\nfor lang in languages:\n    for ds in datasets:\n        if lang in ds.columns:\n            all_texts.extend(ds[lang].dropna().tolist())\n\nall_texts_file = 'all_texts_plain.txt'\nwith open(all_texts_file, 'w', encoding='utf-8') as f:\n    f.write('\\n'.join(all_texts))\n\nspm.SentencePieceTrainer.train(\n    input=all_texts_file,\n    model_prefix='spm_new',\n    vocab_size=9909,\n    character_coverage=1,\n    num_threads=16,\n    train_extremely_large_corpus=False,\n    add_dummy_prefix=False,\n    max_sentencepiece_length=128,\n    max_sentence_length=4192 * 4,\n    pad_id=0,\n    eos_id=1,\n    unk_id=2,\n    bos_id=-1,\n    required_chars=None,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:56:31.825262Z","iopub.execute_input":"2025-06-16T15:56:31.825634Z","iopub.status.idle":"2025-06-16T15:56:32.110626Z","shell.execute_reply.started":"2025-06-16T15:56:31.825598Z","shell.execute_reply":"2025-06-16T15:56:32.109777Z"}},"outputs":[{"name":"stderr","text":"sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: all_texts_plain.txt\n  input_format: \n  model_prefix: spm_new\n  model_type: UNIGRAM\n  vocab_size: 9909\n  self_test_sample_size: 0\n  character_coverage: 1\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 16768\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 128\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: None\n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  seed_sentencepieces_file: \n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 2\n  bos_id: -1\n  eos_id: 1\n  pad_id: 0\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 0\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(185) LOG(INFO) Loading corpus: all_texts_plain.txt\ntrainer_interface.cc(409) LOG(INFO) Loaded all 8975 sentences\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <pad>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(430) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(539) LOG(INFO) all chars count=201414\ntrainer_interface.cc(560) LOG(INFO) Alphabet size=2259\ntrainer_interface.cc(561) LOG(INFO) Final character coverage=1\ntrainer_interface.cc(592) LOG(INFO) Done! preprocessed 8975 sentences.\nunigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=107120\nunigram_model_trainer.cc(312) LOG(INFO) Initialized 19631 seed sentencepieces\ntrainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 8975\ntrainer_interface.cc(609) LOG(INFO) Done! 10959\nunigram_model_trainer.cc(602) LOG(INFO) Using 10959 sentences for EM training\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10176 obj=18.5511 num_tokens=38206 num_tokens/piece=3.75452\nunigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=9575 obj=15.4299 num_tokens=38355 num_tokens/piece=4.00574\ntrainer_interface.cc(687) LOG(INFO) Saving model: spm_new.model\ntrainer_interface.cc(699) LOG(INFO) Saving vocabs: spm_new.vocab\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Add trained tokens to tokenizer \n\ntokenizer_old = NllbTokenizer.from_pretrained(model_name)\n\nsp_trained = spm.SentencePieceProcessor(model_file='spm_new.model')\nadded_spm = sp_pb2_model.ModelProto()\nadded_spm.ParseFromString(sp_trained.serialized_model_proto())\nold_spm_nllb = sp_pb2_model.ModelProto()\nold_spm_nllb.ParseFromString(tokenizer_old.sp_model.serialized_model_proto())\n\nnllb_tokens_set = {p.piece for p in old_spm_nllb.pieces}\nprev_min_score = old_spm_nllb.pieces[-1].score\nfor p in added_spm.pieces:\n    piece = p.piece\n    if p.type != 1:\n        continue\n    if piece not in nllb_tokens_set:\n        new_p = sp_pb2_model.ModelProto().SentencePiece()\n        new_p.piece = piece\n        new_p.score = p.score + prev_min_score\n        old_spm_nllb.pieces.append(new_p)\n\nNEW_SPM_NAME = 'spm_nllb_extended.model'\nwith open(NEW_SPM_NAME, 'wb') as f:\n    f.write(old_spm_nllb.SerializeToString())\n\ntokenizer = NllbTokenizer.from_pretrained(model_name, vocab_file=NEW_SPM_NAME)\nprint(len(tokenizer_old), len(tokenizer))\nadded_vocab = set(tokenizer.get_vocab()).difference(set(tokenizer_old.get_vocab()))\nprint(len(added_vocab))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:56:32.111498Z","iopub.execute_input":"2025-06-16T15:56:32.111710Z","iopub.status.idle":"2025-06-16T15:56:39.587761Z","shell.execute_reply.started":"2025-06-16T15:56:32.111692Z","shell.execute_reply":"2025-06-16T15:56:39.586913Z"}},"outputs":[{"name":"stdout","text":"256204 262441\n6237\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Add trained tokens to model\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\nmodel.resize_token_embeddings(len(tokenizer))\n\nfor t in tqdm(added_vocab):\n    tt = tokenizer_old(t, add_special_tokens=False).input_ids\n    if len(tt) == 0:\n        tt = [tokenizer_old.unk_token_id]\n    idx = tokenizer.convert_tokens_to_ids(t)\n    model.model.shared.weight.data[idx] = model.model.shared.weight.data[tt].mean(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:56:39.588770Z","iopub.execute_input":"2025-06-16T15:56:39.589054Z","iopub.status.idle":"2025-06-16T15:57:32.345819Z","shell.execute_reply.started":"2025-06-16T15:56:39.589030Z","shell.execute_reply":"2025-06-16T15:57:32.344890Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/846 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf774ee734a14a04a401422489d3f283"}},"metadata":{}},{"name":"stderr","text":"2025-06-16 15:56:48.852013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750089409.277891      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750089409.401151      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"(…)1ecdf1e485509035f6b51dfe84f1ada83eefcc42:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75fdd71883974c498503b300a5637e72"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6da99def1562496abecbd653cdb737dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9dc21916e7a42df88ba2f33cea914bf"}},"metadata":{}},{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6237 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d40e749e0a7b48db82256d72145a1c15"}},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"# PART2 Model Fine-tuning","metadata":{}},{"cell_type":"code","source":"# Prepare training pair\n\nLANGS = [('zho', 'zho_Hant'), ('pyu', 'tgl_Latn')]\n\ndfs = [lexicon, sentences, lexicon_en, sentences_en]\ndf_train = pd.concat([df[['pyu', 'zho']] for df in dfs], ignore_index=True)\n\ndef get_batch_pairs(batch_size, data=df_train):\n    (l1, long1), (l2, long2) = random.sample(LANGS, 2)\n    xx, yy = [], []\n    for _ in range(batch_size):\n        item = data.iloc[random.randint(0, len(data)-1)]\n        xx.append(item[l1])\n        yy.append(item[l2])\n    return xx, yy, long1, long2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:57:32.348231Z","iopub.execute_input":"2025-06-16T15:57:32.348861Z","iopub.status.idle":"2025-06-16T15:57:32.361751Z","shell.execute_reply.started":"2025-06-16T15:57:32.348839Z","shell.execute_reply":"2025-06-16T15:57:32.361079Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Trainer setting\n\nmodel.cuda();\noptimizer = Adafactor(\n    [p for p in model.parameters() if p.requires_grad],\n    scale_parameter=False,\n    relative_step=False,\n    lr=1e-4,\n    clip_threshold=1.0,\n    weight_decay=5e-3,\n)\nscheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=2000)\n\nbatch_size = 16\nmax_length = 256\ntraining_steps = 60000\nlosses = []\nMODEL_SAVE_PATH = '/kaggle/working/nllb_extended'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:57:32.362473Z","iopub.execute_input":"2025-06-16T15:57:32.362751Z","iopub.status.idle":"2025-06-16T15:57:36.990294Z","shell.execute_reply.started":"2025-06-16T15:57:32.362726Z","shell.execute_reply":"2025-06-16T15:57:36.989716Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Training\n\ndef cleanup():\n    gc.collect()\n    torch.cuda.empty_cache()\n\nmodel.train()\nx, y, loss = None, None, None\ncleanup()\n\ntq = trange(len(losses), training_steps)\nfor i in tq:\n    xx, yy, lang1, lang2 = get_batch_pairs(batch_size)\n    try:\n        tokenizer.src_lang = lang1\n        x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n        tokenizer.src_lang = lang2\n        y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n        # -100 is a magic value ignored in the loss function\n        # because we don't want the model to learn to predict padding ids\n        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n\n        loss = model(**x, labels=y.input_ids).loss\n        loss.backward()\n        losses.append(loss.item())\n\n        optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n        scheduler.step()\n\n    except RuntimeError as e:\n        optimizer.zero_grad(set_to_none=True)\n        x, y, loss = None, None, None\n        cleanup()\n        print('error', max(len(s) for s in xx + yy), e)\n        continue\n\n    if i % 2000 == 0: # Steps to report loss\n        print(i, np.mean(losses[-1000:]))\n\n    if i % 2000 == 0 and i > 0: # Steps to save model\n        model.save_pretrained(MODEL_SAVE_PATH)\n        tokenizer.save_pretrained(MODEL_SAVE_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:57:36.990997Z","iopub.execute_input":"2025-06-16T15:57:36.991186Z","iopub.status.idle":"2025-06-16T15:57:58.583195Z","shell.execute_reply.started":"2025-06-16T15:57:36.991172Z","shell.execute_reply":"2025-06-16T15:57:58.582108Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/60000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca2e6b2a7f8f4a3dbd25fcbd21001b60"}},"metadata":{}},{"name":"stdout","text":"0 8.860976219177246\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2660687134.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_by_lr_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                 \u001b[0mbeta2t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"decay_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m                 \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfactored\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                     \u001b[0mexp_avg_sq_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"exp_avg_sq_row\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":13},{"cell_type":"markdown","source":"# PART3 NLLB Inference\n\n(This part can be run independent of PART1 PART2 if model is already fine-tuned)","metadata":{}},{"cell_type":"code","source":"# Inference the 1st result with fine-tuned NLLB model\n\nmodel_dir = \"/kaggle/input/nllb-extended/other/45000steps/1/results/nllb_extended\" # Change this to your trained model directory\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir, local_files_only=True).cuda()\ntokenizer = NllbTokenizer.from_pretrained(model_dir, local_files_only=True)\n\ndef clean_output(text):\n    text = text.strip()\n    text = re.sub(r'^[\\\"“”「」『』、,.;:?!]+', '', text)  # Remove starting punctuation\n    text = re.sub(r'[\\\"“”「」『』、,.;:?!]+$', '', text)  # Remove ending punctuation\n    return text if text != \"\" else \"ERROR\"\n\ndef translate(\n    text, src_lang='zho_Hant', tgt_lang='tgl_Latn', \n    a=32, b=3, max_input_length=1024, num_beams=1, **kwargs\n):\n    tokenizer.src_lang = src_lang\n    tokenizer.tgt_lang = tgt_lang\n    inputs = tokenizer(\n        text, return_tensors='pt', padding=True, truncation=True, \n        max_length=max_input_length\n    ).to(model.device)\n\n    model.eval()\n    result = model.generate(\n        **inputs,\n        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n        num_beams=num_beams, **kwargs\n    )\n    decoded = tokenizer.batch_decode(result, skip_special_tokens=True)\n    decoded = [clean_output(t) for t in decoded]\n    return decoded\n\ntranslated_pyu = translate(to_pyu['zho'].tolist(), src_lang='zho_Hant', tgt_lang='pyu_Latn')\ntranslated_zho = translate(to_zho['pyu'].tolist(), src_lang='pyu_Latn', tgt_lang='zho_Hant')\n\nfinal = pd.DataFrame({\n    \"ID\": range(1, len(translated_pyu) + len(translated_zho) + 1),\n    \"answer\": translated_pyu + translated_zho\n})\nfinal['answer'] = final['answer'].fillna('ERROR')\nfinal.to_csv(\"submission_NLLB.csv\", index=False, encoding='utf-8')\nfinal.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:52:18.464474Z","iopub.execute_input":"2025-06-16T15:52:18.464724Z","iopub.status.idle":"2025-06-16T15:53:03.595667Z","shell.execute_reply.started":"2025-06-16T15:52:18.464701Z","shell.execute_reply":"2025-06-16T15:53:03.594898Z"}},"outputs":[{"name":"stderr","text":"2025-06-16 15:52:24.237868: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750089144.441948      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750089144.502056      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   ID                                             answer\n0   1  aaydan ziya na rahan na avukulr mu maw na sala...\n1   2  a maranger na variw, kavang zi sazu na 'azi ta...\n2   3       a Katratripulr na pinarahan在怕 kana sa'ami mu\n3   4       a pakamiisi za lramlram, zapilra、到pilra且are'\n4   5          a meredek harem, kavulay nu kema ku kanmu","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>aaydan ziya na rahan na avukulr mu maw na sala...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>a maranger na variw, kavang zi sazu na 'azi ta...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>a Katratripulr na pinarahan在怕 kana sa'ami mu</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>a pakamiisi za lramlram, zapilra、到pilra且are'</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>a meredek harem, kavulay nu kema ku kanmu</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# PART4 Llama Inference\n\n(This part can be run independently)","metadata":{}},{"cell_type":"code","source":"# Load the Llama model\n\nllama3 = Llama(\n    \"/kaggle/working/Llama-3.2-3B-Instruct-Q6_K.gguf\",\n    verbose=False,\n    n_gpu_layers=-1,\n    n_ctx=30000,\n)\n\ndef generate_response(_model: Llama, _messages: str) -> str:\n    _output = _model.create_chat_completion(\n        _messages,\n        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n        max_tokens=512,\n        temperature=0,\n        repeat_penalty=2.0,\n    )[\"choices\"][0][\"message\"][\"content\"]\n    return _output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:41:29.263060Z","iopub.execute_input":"2025-06-16T15:41:29.263360Z","iopub.status.idle":"2025-06-16T15:41:31.460215Z","shell.execute_reply.started":"2025-06-16T15:41:29.263338Z","shell.execute_reply":"2025-06-16T15:41:31.459294Z"}},"outputs":[{"name":"stderr","text":"llama_new_context_with_model: n_ctx_per_seq (30016) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Reference translation pair preparation\n\ndfs = [lexicon, sentences, lexicon_en, sentences_en]\ncombined = pd.concat(\n    [df[['pyu', 'zho']] for df in dfs],\n    ignore_index=True\n)\ncombined = combined.drop_duplicates().reset_index(drop=True)\npairs = combined['zho'] + ' = ' + combined['pyu']\nreference_text = \"\\n\".join(pairs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:41:33.651807Z","iopub.execute_input":"2025-06-16T15:41:33.652108Z","iopub.status.idle":"2025-06-16T15:41:33.678850Z","shell.execute_reply.started":"2025-06-16T15:41:33.652084Z","shell.execute_reply":"2025-06-16T15:41:33.678014Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Reference puyuma grammar book\n\ngrammar_book = \"第三章 詞彙與構詞本章主要討論知本卑南語的詞彙結構及主要構詞方式。3.1 主要構詞單位3.1.1 詞及詞素詞 (Word)：是句子結構中的最小單位 。 音節多寡：詞可以是單音節（如 mu「當...的時候」）、雙音節（如 ru.ma「房子」、pa.kan「餵」）、三音節（如 mu.di.ngan「臉」）或四音節以上（如 pu.a.li.ma「戴戒指」） 。組成成份：有些詞由單一成分組成，無法再分解，稱為單純詞（如 ru.ma「房子」） 。有些則由兩個（如 pa+kan「餵(使...吃)」）或更多成分組成，稱為複雜詞 。語意與句法功能：具有實質語意的稱為實詞（如名詞 pu.ran「檳榔」、動詞 pakan「餵」），屬於開放性詞類 。具有句法功能的稱為虛詞或功能詞（如格位標記 za「斜格」、代名詞 inku「我,主格」），屬於封閉性詞類 。詞素 (Morpheme)：是語言系統中具有意義或語法功能的最小單位 。 自由詞素：可以獨立存在的詞素，如 velrvelr「香蕉」、kayan「坐」、inku「我、主格」 。附著詞素：一定要附加在某個詞上，不能單獨使用的詞素 。可分為詞綴（如 ki-「取得」、pa-「使...」）和依附詞（如代名詞 =ku「我 主格」） 。3.1.2 詞根及詞幹詞根 (Root)：是最小且具有意義的詞素，不包括任何附加成份（如重疊或詞綴） 。例如 matra「眼睛」是詞根，因為無法再切割成有意義的 *ma- 或 *-tra 。詞根不分長短，例如 velrvelr「香蕉」也是一個詞根 。詞幹 (Stem)：可以單純由一個詞根構成，也可以包含詞根再加上詞綴 。例如，在 pa-nadam「教」中，詞幹是 nadam「學習」；而在 ki-pa-nadam「受教」中，詞幹則是 pa-nadam 。3.1.3 詞綴及依附詞詞綴與依附詞都不能單獨使用。在本書中，詞綴用連字符號 - 標示，依附詞則用等號 = 標示 。例如：在 tu veray-ay=ku「他給我」一句中，-ay 是詞綴，=ku 是依附詞 。臺灣南島語在加詞綴的過程中，通常會影響重音。例如 inavă「好」加上後綴 -an 後，重音會移至最後音節，變成 inava-án 。詞綴可分為兩類：屈折詞綴：附加在特定詞類上，用來表示語法功能（如語氣、時貌），但不改變該詞的詞類。例如：動詞 pukpuk「打」→ pukpuk-u!「打！」（命令式動詞） 。衍生詞綴：會產生不同的語意並（或）造成詞類的改變。例如：名詞 avay「年糕」→ 動詞 tu-avay「做年糕」；動詞 ekan「吃」→ 名詞 a-ekan-an「食物」 。依附詞與詞綴的不同在於，依附詞不選擇其「寄主詞」的詞類或語意，通常依附於句中第一個成分。例如，依附詞 =ku「我」可以依附於動詞、名詞或否定詞 。 mapungaw=ku.（我頭暈）vs. a sinsi=ku.（我是老師） 'azi=ku mapungaw.（我沒頭暈）vs. melri=ku a sinsi.（我不是老師） 3.1.4 同位詞 (Allomorph)同位詞是一個詞素在不同語音環境下的變體 。主事焦點中綴 <em> 有三個同位詞：m-、me- 及 <en> 。 m-：出現在母音開頭的動詞上，如 m-abak「裝」、m-alak「拿」 。me-：出現在 n 及 ng 開頭的動詞上，如 me-na'u「看」、me-ngara「等」 。<en>：出現在 v 及 p 開頭的動詞上，如 v<en>usus「騙」、p<en>a'ing「打噴嚏」 。<em>：出現在其餘語音環境，如 k<em>ayan「坐下」、tr<em>evel「理髮」 。表完成的中綴 <in> 有兩個同位詞：in- 及 ni- 。 in-：出現在母音開頭的動詞上，如 in-abak「被裝了的」、in-alak「被拿了的」 。ni-：出現在 n 及 ng 開頭的動詞上，如 ni-na'u「被看了的」、ni-ngara「被等了的」 。<in>：出現在其餘語音環境，如 v<in>usus「被騙了的」、k<in>ayan「被坐下」 。3.2 構詞方法知本卑南語的主要構詞方法包括加綴、重疊及複合 。3.2.1 加綴 (Affixation)前綴：改變詞類（名詞 → 動詞） ki-：「取得」，如 ki-paisu「要錢」 。mi-：「穿、戴、帶、有」，如 mi-kavang「穿衣」、mi-paisu「有錢」 。mutu-：「變成」，如 mutu-trau「變成人」 。tara-：「使用」，如 tara-puyuma「說卑南語」 。tu-：「製造、產生」，如 tua-avay「做糯米糕」 。tinu-：「模擬」，如 tinu-maizang「實習長老」 。不改變詞類 mare-（名詞→名詞）：「互相」，如 mare-wadi「兄弟姊妹」 。kara-（動詞→動詞）：「一起」，如 kara-kayan「坐在一起」 。pa-（動詞→動詞）：「使、讓」，如 pa-ekan「餵、使吃」 。mara-（動詞→動詞）：「比較」，如 mara-lriketri「較短」 。中綴 ： <in>：表示「完成」，如 d<in>away「做好的」 。<em>：表示「主事焦點」，如 k<em>ayan「坐」 。後綴 ： -an：將動詞轉為名詞，表「地方」，如 takesi-an「學校」、tra'i-tra'i-an「廁所」 。環綴 ： ka-...-an：表示「做...的時期」或「真正的...」，如 ka-salem-an「種植的季節」、ka-ruma-an「主屋」 。<in>...anan：表示「...的成員」，如 z<in>pekalr-anan「村民」 。3.2.2 重疊 (Reduplication)Ca-重疊：重疊詞根倒數第二音節的輔音再加上母音 /a/ 。 在數詞上表達「數人」：如 zuwa「二」 → za-zuwa「兩人」 。在動詞上表示「進行」或「非實現」：如 senay「唱」 → s<em>a-senay「正在唱」 。表示「互相」：如 karatr「咬」 → ma-ka-karatr「互咬」 。在名詞上表示「通稱」或「多數」：如 trau「人」 → tra-trau-an「人類」 。形成表示「處所」的名詞：如 dirus「洗澡」 → da-dirus-an「洗澡間」 。形成表示「工具」的名詞：如 ngisil「刷」 → nga-ngisil「牙刷」 。雙音節重疊：重疊字根倒數兩個音節 。 加在名詞表示「複數」或「總稱」：如 zenan → zena-zenan「山脈」、tralun「草」→ tralu-tralun「草叢」 。加在動態動詞表「動作重複」：如 me-na'u「看」 → me-nau-na'u「不斷地看」 。加在靜態動詞「加重程度」：如 dawilr「遠」 → dawidawilr「很遠」 。複雜重疊：結合兩種以上方式的重疊 。例如 wari「天」 → wa-wari-wari「每天」 。3.3 擬聲詞擬聲詞是用聲音摹仿事物、動作或自然界聲音的詞彙 。動物：ngiaw「貓」、up'up「牛蛙」、maymay「鴨」、wa wa「烏鴉」、tutur「鴿子」 。昆蟲：tengteng「蜻蜓」、kengkeng「蚊子」 。動詞 (模擬動作聲音)：tiktik「雕刻聲」、tuktuk「鐵鎚聲」、taktak「砍樹聲」、pukpuk「用棍子打孩子聲」 。動詞 (模擬自然界聲音)：zerung「打雷聲」、treli「閃電」 。3.4 借詞知本卑南語的借詞來源有日語、台語及中文 。日語借詞：kupu「杯子」、layta「打火機」、sulippa「拖鞋」、iga「電影」、kikay「機器」、sinsi「老師」、hikoki「飛機」、dingwa「電話」、tuki「時鐘/手錶」、wasabi「芥末」、tomato「番茄」、sibiru「西裝」 。台語借詞：dolayba「螺ising起子」、tangsuy「雨衣」、ising「醫生」、voksi「牧師」、tu「桌子」、tawyu「醬油」、pisay「白菜」、kiw「茄子」 。3.5 詞類類別詞類可分為成員數量有限的封閉性詞類（如代名詞、副詞）和成員沒有限制的開放性詞類（如動詞、名詞） 。3.5.1 開放性詞類動詞和名詞的區分：從構詞上區分不易，句法上的證據比較可靠 。 指示代名詞可以出現在名詞前（ini na alrak「這個孩子」），但不能出現在動詞前 。自由式的代名詞可以出現在名詞前，但不能在動詞前。例如在 tu ngarayaw tu sinsi「他等他的老師」中，tu sinsi 可以被 nantu sinsi 取代，但 tu ngarayaw 不能被取代 。名詞用 melri 來否定（melri a sinsi intaw.「他不是老師」），動詞用 'azi 來否定（'azi maekan za vulraw.「他不吃魚」） 。名詞：可分為三類，各由不同格位標記來標示 。 人稱專有名詞：包含人名及親屬稱謂，有單複數之分。如 zua i tainataw.「他的媽媽來了。」 。處所名詞：如 adawilr i Tayhok.「台北很遠。」 。一般名詞：有「限定」與「非限定」之分。如 ulra a trau i ruma.「房子裡有人。」 。動詞：動詞上的焦點詞綴決定了主語的語意角色，主要有四種焦點：主事者 (<em>)、受事者 (-aw)、處所 (-ay)、受惠者/工具 (-anay) 。知本卑南語沒有獨立的「形容詞」詞類，其功能由靜態動詞（如「喜歡」、「害怕」）承擔 。動詞分為動態動詞和靜態動詞。動態動詞通常帶 <em>（或其同位詞），而靜態動詞帶 ma- 。兩者在命令句、否定句、非實現貌及使役句中有不同的標記方式 。3.5.2 封閉性詞類格位標記：出現在名詞或名詞組之前，標示其語意角色或文法關係 。人稱代名詞：指「我」、「你」、「他」等。第一人稱複數常區分「包含式」（咱們）和「排除式」（我們） 。指定代名詞：即指示代名詞，可單獨使用或修飾名詞，形式可能因與說話者距離、是否可見、單複數等因素而異 。疑問詞：用於構成特殊問句，如「誰」、「什麼」、「何處」等 。數字：分為基數詞與序數詞等 。詞組標記和子句標記：詞組標記：如連繫詞 na，常出現在名詞之間。例：zua na tatelru na trau.「那三個人來了。」 。並列連詞如 zi「和」。例：vi'as na kadaw zi, pitalupung...「太陽熱，而且要戴帽子...」 。子句標記：如主題標記 mu 和從屬連詞 nu「當」。例：na vavuy mu, tu kuwangaw ni ama za kuwang.「（那隻）山豬，爸爸用槍射了。」 。感嘆詞：表示驚訝、痛苦、悲傷等感情，如 iwa「唉呀」 。\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:41:35.676851Z","iopub.execute_input":"2025-06-16T15:41:35.677141Z","iopub.status.idle":"2025-06-16T15:41:35.683915Z","shell.execute_reply.started":"2025-06-16T15:41:35.677117Z","shell.execute_reply":"2025-06-16T15:41:35.683136Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Define LLM agent\n\ndef clean(text):\n    return re.sub(r\"[^\\w\\s]\", \"\", text).strip()\n    \ndef is_valid_word(word: str, min_alpha: int = 4) -> bool:\n    num_alpha = sum(c.isalpha() for c in word)\n    return num_alpha >= min_alpha\n\ndef fuzzy_search_entries(reference_text: str, keyword: str, top_k: int = 50) -> str:\n    keyword = clean(keyword)\n\n    ignore_words = {\"的\", \"一\", \"在\", \" \"}\n    raw_words = jieba.cut(keyword)\n\n    words = []\n    for w in raw_words:\n        if w in ignore_words:\n            continue\n        if re.fullmatch(r'[a-zA-Z\\']+', w):\n            if not is_valid_word(w):\n                continue\n        words.append(w)\n\n    if not words:\n        return \"無可參考條目\"\n\n    matches = []\n    for line in reference_text.split(\"\\n\"):\n        cleaned_line = clean(line)\n        if any(w in cleaned_line for w in words):\n            matches.append(line)\n\n    return \"\\n\".join(matches[:top_k]) if matches else \"找不到相關翻譯資料。\"\n\nclass LLMAgent:\n    def __init__(self, role_description, task_description, references=None):\n        self.role_description = role_description\n        self.task_description = task_description\n        self.references = references\n\n    def inference(self, message: str, ref: str = None, nllb: str = None) -> str:\n        used_references = ref if ref is not None else self.references\n        messages = [\n            {\"role\": \"system\", \"content\": self.role_description},\n            {\"role\": \"user\", \"content\": f\"以下是卑南語的文法說明，可作為翻譯參考：「{grammar_book}」\"},\n            {\"role\": \"user\", \"content\": f\"以下是相關詞彙的翻譯，可作為翻譯參考：「{used_references}」\"},\n            {\"role\": \"user\", \"content\": f\"以下是NLLB模型輸出的翻譯結果，可作為翻譯參考：「{nllb}」\"}, # if no output from previous part, comment this\n            {\"role\": \"user\", \"content\": f\"{self.task_description}：「{message}」\"},\n        ]\n        return generate_response(llama3, messages)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:41:37.300641Z","iopub.execute_input":"2025-06-16T15:41:37.301140Z","iopub.status.idle":"2025-06-16T15:41:37.309930Z","shell.execute_reply.started":"2025-06-16T15:41:37.301111Z","shell.execute_reply":"2025-06-16T15:41:37.308987Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# LLM agent prompting\n\ntranstopyu_agent = LLMAgent(\n    role_description=\"你是聰明的語言模型，擅長翻譯繁體中文與卑南語，幫我參考以下資料後，利用特性嘗試翻譯文字至卑南語，並只保留卑南語的翻譯結果，不須說明。\",\n    task_description=\"翻譯以下文字為卑南語，並只保留翻譯內容\",\n)\n\ntranstozho_agent = LLMAgent(\n    role_description=\"你是聰明的語言模型，擅長翻譯繁體中文與卑南語，幫我參考以下資料後，利用特性嘗試翻譯文字至繁體中文，並只保留繁體中文的翻譯結果，不須說明。\",\n    task_description=\"翻譯以下文字為繁體中文，並只保留翻譯內容\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:41:40.638191Z","iopub.execute_input":"2025-06-16T15:41:40.638944Z","iopub.status.idle":"2025-06-16T15:41:40.643378Z","shell.execute_reply.started":"2025-06-16T15:41:40.638919Z","shell.execute_reply":"2025-06-16T15:41:40.642420Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Inference the final result with llama\n\nnllb_df = pd.read_csv(\"/kaggle/input/nllboutput/submission (3).csv\") # Change this to your previous output file\nnllb_outputs = nllb_df['answer'].tolist()\n\ndef translate_with_agent(texts, src_lang='zho'):\n    results = []\n    for i, text in enumerate(tqdm(texts, desc=f\"Translating from {src_lang}\")):\n        ref = fuzzy_search_entries(reference_text, text)\n        try:\n            nllb_result = nllb_outputs[i] if i < len(nllb_outputs) else \"\"\n            if src_lang == 'zho':\n                result = transtopyu_agent.inference(text, ref=ref, nllb=nllb_result)\n            else:\n                result = transtozho_agent.inference(text, ref=ref, nllb=nllb_result)\n        except Exception as e:\n            result = \"ERROR\"\n        cleaned_result = result.replace(\"\\n\", \"\").replace(\"\\r\", \"\").strip()\n        results.append(cleaned_result)\n    return results\n\ntranslated_pyu = translate_with_agent(to_pyu['zho'].tolist(), src_lang='zho')\ntranslated_zho = translate_with_agent(to_zho['pyu'].tolist(), src_lang='pyu')\n\nfinal = pd.DataFrame({\n    \"ID\": range(1, len(translated_pyu) + len(translated_zho) + 1),\n    \"answer\": translated_pyu + translated_zho\n})\nfinal['answer'] = final['answer'].fillna('ERROR')\nfinal.to_csv(\"submission_FINAL.csv\", index=False, encoding='utf-8')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T15:41:43.365715Z","iopub.execute_input":"2025-06-16T15:41:43.366004Z","iopub.status.idle":"2025-06-16T15:48:44.312473Z","shell.execute_reply.started":"2025-06-16T15:41:43.365983Z","shell.execute_reply":"2025-06-16T15:48:44.311529Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Translating from zho:   0%|          | 0/60 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d19496e6afe40dba85b2f1e18aa0354"}},"metadata":{}},{"name":"stderr","text":"Building prefix dict from the default dictionary ...\nDumping model to file cache /tmp/jieba.cache\nLoading model cost 0.721 seconds.\nPrefix dict has been built successfully.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Translating from pyu:   0%|          | 0/145 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adffcadc98e54e5792eb6f32956a4eb9"}},"metadata":{}}],"execution_count":10}]}