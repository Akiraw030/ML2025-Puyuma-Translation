{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11640807,"sourceType":"datasetVersion","datasetId":7304401},{"sourceId":12132896,"sourceType":"datasetVersion","datasetId":7640588},{"sourceId":431918,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":352091,"modelId":373359}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:34:36.460005Z","iopub.execute_input":"2025-06-11T17:34:36.460290Z","iopub.status.idle":"2025-06-11T17:34:36.683232Z","shell.execute_reply.started":"2025-06-11T17:34:36.460265Z","shell.execute_reply":"2025-06-11T17:34:36.682354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Package download\n\n!pip install sentencepiece -q\n!pip install transformers -q\n!pip install datasets -q\n!pip install peft -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T13:22:36.856681Z","iopub.execute_input":"2025-06-11T13:22:36.857362Z","iopub.status.idle":"2025-06-11T13:23:57.083662Z","shell.execute_reply.started":"2025-06-11T13:22:36.857340Z","shell.execute_reply":"2025-06-11T13:23:57.082765Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part1","metadata":{}},{"cell_type":"code","source":"# Nllb loading\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel_name = \"facebook/nllb-200-distilled-600M\"\n# model_name = \"facebook/nllb-200-3.3B\" # Larger model\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ntokenizer.src_lang = \"zho_Hant\"\ntokenizer.tgt_lang = \"tgl_Latn\"\n# zho_Hant for Chinese traditional\n# eng_Latn for English\n# tgl_Latn for Puyuma (Use existing language tag)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:06:21.982616Z","iopub.execute_input":"2025-06-10T12:06:21.982842Z","iopub.status.idle":"2025-06-10T12:07:14.333970Z","shell.execute_reply.started":"2025-06-10T12:06:21.982819Z","shell.execute_reply":"2025-06-10T12:07:14.333138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data into dataframes\n\nimport pandas as pd\n\nlexicon = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/lexicon_no_en.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nlexicon.columns = ['pyu', 'zho']\n\nlexicon_en = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/lexicon.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nlexicon_en.columns = ['pyu', 'eng', 'zho']\n\nsentences = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/sentences_no_en.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nsentences.columns = ['pyu', 'zho']\n\nsentences_en = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/sentences.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nsentences_en.columns = ['pyu', 'eng', 'zho']\n\n#lexicon.sample(5)\n#lexicon_en.sample(10)\n#sentences.sample(5)\n#sentences_en.sample(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T13:25:27.431218Z","iopub.execute_input":"2025-06-11T13:25:27.431776Z","iopub.status.idle":"2025-06-11T13:25:27.760134Z","shell.execute_reply.started":"2025-06-11T13:25:27.431751Z","shell.execute_reply":"2025-06-11T13:25:27.759275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Testing the performances of original tokenization\n\nimport re\n\ndef word_tokenize(text):\n    \n    return re.findall('(\\w+|[^\\w\\s])', text)\n\ndef df_tokenize(df):\n    df['pyu_toks'] = df.pyu.apply(tokenizer.tokenize)\n    df['zho_toks'] = df.zho.apply(tokenizer.tokenize)\n    df['pyu_words'] = df.pyu.apply(word_tokenize)\n    df['zho_words'] = df.zho.apply(word_tokenize)\n    \n    return df\n\ndef cal_tokperword(df):\n\n    stats = df[['pyu_toks', 'zho_toks', 'pyu_words', 'zho_words']].map(len).describe()\n    print(stats.pyu_toks['mean'] / stats.pyu_words['mean'])\n    print(stats.zho_toks['mean'] / stats.zho_words['mean'])\n\n    return stats\n\ndef check_unk(df, column):\n\n    texts_with_unk = [\n        text for text in df[column]\n        if tokenizer.unk_token_id in tokenizer(text).input_ids\n    ]\n    print(len(texts_with_unk))\n\nlexicon = df_tokenize(lexicon)\nlexicon_en = df_tokenize(lexicon_en)\nsentences = df_tokenize(sentences)\nsentences_en = df_tokenize(sentences_en)\n\nprint(\"toks per word of lexicon:\")\nstats_lexicon = cal_tokperword(lexicon)\nprint(\"toks per word of lexicon_en:\")\nstats_lexicon = cal_tokperword(lexicon_en)\nprint(\"toks per word of sentences:\")\nstats_sentences = cal_tokperword(sentences)\nprint(\"toks per word of sentences_en:\")\nstats_sentences = cal_tokperword(sentences_en)\n\nprint(\"total unk in lexicon zho:\")\ncheck_unk(lexicon, \"zho\")\nprint(\"total unk in lexicon pyu:\")\ncheck_unk(lexicon, \"pyu\")\nprint(\"total unk in lexicon_en zho:\")\ncheck_unk(lexicon_en, \"zho\")\nprint(\"total unk in lexicon_en pyu:\")\ncheck_unk(lexicon_en, \"pyu\")\nprint(\"total unk in sentences zho:\")\ncheck_unk(sentences, \"zho\")\nprint(\"total unk in sentences pyu:\")\ncheck_unk(sentences, \"pyu\")\nprint(\"total unk in sentences_en zho:\")\ncheck_unk(sentences_en, \"zho\")\nprint(\"total unk in sentences pyu:\")\ncheck_unk(sentences_en, \"pyu\")\n\n#show datas\n#lexicon.sample(10)\n#sentences.sample(10)\n#stats_lexicon\n#stats_sentences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:07:14.426649Z","iopub.execute_input":"2025-06-10T12:07:14.426945Z","iopub.status.idle":"2025-06-10T12:07:22.933167Z","shell.execute_reply.started":"2025-06-10T12:07:14.426920Z","shell.execute_reply":"2025-06-10T12:07:22.932286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training tokenizer for missing tokens\n\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport re\nfrom collections import Counter\nimport sentencepiece as spm\nfrom datasets import load_dataset\n\nall_texts = lexicon['zho'].dropna().tolist() + sentences['zho'].dropna().tolist() + lexicon_en['zho'].dropna().tolist() + sentences_en['zho'].dropna().tolist() + lexicon['pyu'].dropna().tolist() + sentences['pyu'].dropna().tolist() + lexicon_en['pyu'].dropna().tolist() + sentences_en['pyu'].dropna().tolist()\n\nall_texts_file = 'all_texts_plain.txt'\nwith open(all_texts_file, 'w', encoding='utf-8') as f:\n    for text in all_texts:\n        print(text, file=f)\n\nrequired_chars = set()\n\nfor text in tqdm(all_texts):\n    for char in text:\n        tokens = tokenizer.tokenize(char)\n        if tokens == ['▁', '<unk>']:\n            required_chars.add(char)\n\nrequired_chars_str = \"\".join(sorted(list(required_chars)))\nprint(f\"需要強制包含的單字元: {required_chars_str}\")\n\nspm.SentencePieceTrainer.train(\n    input=all_texts_file,\n    model_prefix='spm_new',\n    vocab_size=5800,\n    character_coverage=1,\n    num_threads=16,\n    train_extremely_large_corpus=False,\n    add_dummy_prefix=False,\n    max_sentencepiece_length=128,\n    max_sentence_length=4192 * 4,\n    pad_id=0,\n    eos_id=1,\n    unk_id=2,\n    bos_id=-1,\n    required_chars=required_chars_str,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:07:22.934027Z","iopub.execute_input":"2025-06-10T12:07:22.934391Z","iopub.status.idle":"2025-06-10T12:07:31.042169Z","shell.execute_reply.started":"2025-06-10T12:07:22.934366Z","shell.execute_reply":"2025-06-10T12:07:31.041208Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add trained tokens to tokenizer and model\n\nfrom sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\nfrom transformers import NllbTokenizer\n\nmodel_name = 'facebook/nllb-200-distilled-600M'\ntokenizer_nllb = NllbTokenizer.from_pretrained(model_name)\n\nsp_trained = spm.SentencePieceProcessor(model_file='spm_new.model')\nadded_spm = sp_pb2_model.ModelProto()\nadded_spm.ParseFromString(sp_trained.serialized_model_proto())\nold_spm_nllb = sp_pb2_model.ModelProto()\nold_spm_nllb.ParseFromString(tokenizer_nllb.sp_model.serialized_model_proto())\n\nnllb_tokens_set = {p.piece for p in old_spm_nllb.pieces}\nprev_min_score = old_spm_nllb.pieces[-1].score\nfor p in added_spm.pieces:\n    piece = p.piece\n    if p.type != 1:\n        continue\n    if piece not in nllb_tokens_set:\n        new_p = sp_pb2_model.ModelProto().SentencePiece()\n        new_p.piece = piece\n        new_p.score = p.score + prev_min_score\n        old_spm_nllb.pieces.append(new_p)\n\nNEW_SPM_NAME = 'spm_nllb_extended_268k.model'\nwith open(NEW_SPM_NAME, 'wb') as f:\n    f.write(old_spm_nllb.SerializeToString())\n\ntokenizer = NllbTokenizer.from_pretrained(model_name, vocab_file='spm_new.model')\nprint(len(tokenizer_nllb), len(tokenizer))\nadded_vocab = set(tokenizer.get_vocab()).difference(set(tokenizer_nllb.get_vocab()))\n#print(added_vocab)(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:07:31.043323Z","iopub.execute_input":"2025-06-10T12:07:31.044077Z","iopub.status.idle":"2025-06-10T12:07:37.986497Z","shell.execute_reply.started":"2025-06-10T12:07:31.044034Z","shell.execute_reply":"2025-06-10T12:07:37.985867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PART2","metadata":{}},{"cell_type":"code","source":"from transformers.optimization import Adafactor\nfrom transformers import get_constant_schedule_with_warmup\nmodel.cuda();\noptimizer = Adafactor(\n    [p for p in model.parameters() if p.requires_grad],\n    scale_parameter=False,\n    relative_step=False,\n    lr=1e-4,\n    clip_threshold=1.0,\n    weight_decay=1e-3,\n)\nscheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=1000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:07:37.987263Z","iopub.execute_input":"2025-06-10T12:07:37.987550Z","iopub.status.idle":"2025-06-10T12:07:39.283558Z","shell.execute_reply.started":"2025-06-10T12:07:37.987525Z","shell.execute_reply":"2025-06-10T12:07:39.282928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nLANGS = [('zho', 'zho_Hant'), ('pyu', 'tgl_Latn')]\n\ndfs = [lexicon, sentences, lexicon_en, sentences_en]\ndf_train = pd.concat([df[['pyu', 'zho']] for df in dfs], ignore_index=True)\n\ndef get_batch_pairs(batch_size, data=df_train):\n    (l1, long1), (l2, long2) = random.sample(LANGS, 2)\n    xx, yy = [], []\n    for _ in range(batch_size):\n        item = data.iloc[random.randint(0, len(data)-1)]\n        xx.append(item[l1])\n        yy.append(item[l2])\n    return xx, yy, long1, long2\n\nprint(get_batch_pairs(1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:07:39.284303Z","iopub.execute_input":"2025-06-10T12:07:39.284556Z","iopub.status.idle":"2025-06-10T12:07:39.294548Z","shell.execute_reply.started":"2025-06-10T12:07:39.284529Z","shell.execute_reply":"2025-06-10T12:07:39.293791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 16  # 32 already doesn't fit well to 15GB of GPU memory\nmax_length = 128  # token sequences will be truncated\ntraining_steps = 50000  # Usually, I set a large number of steps,\n# and then just interrupt the training manually\nlosses = []  # with this list, I do very simple tracking of average loss\nMODEL_SAVE_PATH = '/kaggle/working/nllb_extended'  # on my Google drive","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:07:39.296771Z","iopub.execute_input":"2025-06-10T12:07:39.296995Z","iopub.status.idle":"2025-06-10T12:07:39.318919Z","shell.execute_reply.started":"2025-06-10T12:07:39.296978Z","shell.execute_reply":"2025-06-10T12:07:39.318383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport torch\nimport numpy as np\nfrom tqdm.auto import tqdm, trange\n\ndef cleanup():\n    gc.collect()\n    torch.cuda.empty_cache()\n\nmodel.train()\nx, y, loss = None, None, None\ncleanup()\n\ntq = trange(len(losses), training_steps)\nfor i in tq:\n    xx, yy, lang1, lang2 = get_batch_pairs(batch_size)\n    try:\n        tokenizer.src_lang = lang1\n        x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n        tokenizer.src_lang = lang2\n        y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n        # -100 is a magic value ignored in the loss function\n        # because we don't want the model to learn to predict padding ids\n        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n\n        loss = model(**x, labels=y.input_ids).loss\n        loss.backward()\n        losses.append(loss.item())\n\n        optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n        scheduler.step()\n\n    except RuntimeError as e:  # usually, it is out-of-memory\n        optimizer.zero_grad(set_to_none=True)\n        x, y, loss = None, None, None\n        cleanup()\n        print('error', max(len(s) for s in xx + yy), e)\n        continue\n\n    if i % 1000 == 0:\n        # each 1000 steps, I report average loss at these steps\n        print(i, np.mean(losses[-1000:]))\n\n    if i % 1000 == 0 and i > 0:\n        model.save_pretrained(MODEL_SAVE_PATH)\n        tokenizer.save_pretrained(MODEL_SAVE_PATH)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-10T15:43:10.432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PART3","metadata":{}},{"cell_type":"code","source":"from transformers import NllbTokenizer, AutoModelForSeq2SeqLM\nmodel_dir = '/kaggle/input/nllb-extended/other/45000steps/1/results/nllb_extended'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir, local_files_only=True).cuda()\ntokenizer = NllbTokenizer.from_pretrained(model_dir, vocab_file='/kaggle/input/nllb-extended/other/45000steps/1/results/spm_new.model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T09:24:30.128985Z","iopub.execute_input":"2025-06-11T09:24:30.129711Z","iopub.status.idle":"2025-06-11T09:24:30.198055Z","shell.execute_reply.started":"2025-06-11T09:24:30.129689Z","shell.execute_reply":"2025-06-11T09:24:30.197510Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate(\n    text, src_lang='zho_Hant', tgt_lang='tgl_Latn', \n    a=32, b=3, max_input_length=1024, num_beams=1, **kwargs\n):\n    \"\"\"Turn a text or a list of texts into a list of translations\"\"\"\n    tokenizer.src_lang = src_lang\n    tokenizer.tgt_lang = tgt_lang\n    inputs = tokenizer(\n        text, return_tensors='pt', padding=True, truncation=True, \n        max_length=max_input_length\n    )\n    model.eval() # turn off training mode\n    result = model.generate(\n        **inputs.to(model.device),\n        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n        num_beams=num_beams, **kwargs\n    )\n    return tokenizer.batch_decode(result, skip_special_tokens=True)\n\nt = '我也沒帶錢耶!'\nprint(translate(t, 'zho_Hant', 'tgl_Latn'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T09:29:14.531400Z","iopub.execute_input":"2025-06-11T09:29:14.531700Z","iopub.status.idle":"2025-06-11T09:29:14.689735Z","shell.execute_reply.started":"2025-06-11T09:29:14.531679Z","shell.execute_reply":"2025-06-11T09:29:14.689087Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PART Additional","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nmodel_dir = \"/kaggle/input/nllb-extended/other/45000steps/1/results/nllb_extended\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir, local_files_only=True).to(\"cuda\")\n\ndef translate(\n    text, src_lang='zho_Hant', tgt_lang='tgl_Latn', \n    a=32, b=3, max_input_length=1024, num_beams=1, **kwargs\n):\n    tokenizer.src_lang = src_lang\n    tokenizer.tgt_lang = tgt_lang\n    inputs = tokenizer(\n        text, return_tensors='pt', padding=True, truncation=True, \n        max_length=max_input_length\n    ).to(model.device)\n\n    model.eval()\n    result = model.generate(\n        **inputs,\n        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n        num_beams=num_beams, **kwargs\n    )\n    decoded = tokenizer.batch_decode(result, skip_special_tokens=True)\n\n    # 如果解碼結果是空字串，就用 'ERROR' 代替\n    decoded = [t if t.strip() != \"\" else \"ERROR\" for t in decoded]\n\n    return decoded\n\nto_pyu = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/zh_to_pyu_test.csv', header=None)[0]\nto_zho = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/pyu_to_zh_test.csv', header=None)[0]\n\ntranslated_pyu = translate(to_pyu.tolist(), src_lang='zho_Hant', tgt_lang='pyu_Latn')\ntranslated_zho = translate(to_zho.tolist(), src_lang='pyu_Latn', tgt_lang='zho_Hant')\n\nfinal = pd.DataFrame({\n    \"ID\": range(1, len(translated_pyu) + len(translated_zho) + 1),\n    \"answer\": translated_pyu + translated_zho\n})\nfinal['answer'] = final['answer'].fillna('ERROR')\nfinal.to_csv(\"submission.csv\", index=False, encoding='utf-8')\nfinal.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T13:35:15.371620Z","iopub.execute_input":"2025-06-11T13:35:15.372294Z","iopub.status.idle":"2025-06-11T13:35:23.584000Z","shell.execute_reply.started":"2025-06-11T13:35:15.372274Z","shell.execute_reply":"2025-06-11T13:35:23.583112Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PART4","metadata":{}},{"cell_type":"code","source":"!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122 -q\n!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean -q\n\n!wget https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K.gguf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:34:46.595268Z","iopub.execute_input":"2025-06-11T17:34:46.596160Z","iopub.status.idle":"2025-06-11T17:35:24.315590Z","shell.execute_reply.started":"2025-06-11T17:34:46.596121Z","shell.execute_reply":"2025-06-11T17:35:24.314931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from llama_cpp import Llama\n\n# Load the model onto GPU\nllama3 = Llama(\n    \"/kaggle/working/Llama-3.2-3B-Instruct-Q6_K.gguf\",\n    verbose=False,\n    n_gpu_layers=-1,\n    n_ctx=30000,\n)\n\ndef generate_response(_model: Llama, _messages: str) -> str:\n    \n    _output = _model.create_chat_completion(\n        _messages,\n        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n        max_tokens=512,\n        temperature=0,\n        repeat_penalty=2.0,\n    )[\"choices\"][0][\"message\"][\"content\"]\n    return _output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:35:57.640461Z","iopub.execute_input":"2025-06-11T17:35:57.640741Z","iopub.status.idle":"2025-06-11T17:36:00.046088Z","shell.execute_reply.started":"2025-06-11T17:35:57.640716Z","shell.execute_reply":"2025-06-11T17:36:00.045516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport asyncio\n\n# === 資料載入 ===\nlexicon = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/lexicon_no_en.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nlexicon.columns = ['pyu', 'zho']\n\nlexicon_en = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/lexicon.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nlexicon_en.columns = ['pyu', 'eng', 'zho']\n\nsentences = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/sentences_no_en.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nsentences.columns = ['pyu', 'zho']\n\nsentences_en = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/sentences.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nsentences_en.columns = ['pyu', 'eng', 'zho']\n\n# === 組合與清洗資料 ===\ndf1 = lexicon[['pyu', 'zho']]\ndf2 = lexicon_en[['pyu', 'zho']]\ndf3 = sentences[['pyu', 'zho']]\ndf4 = sentences_en[['pyu', 'zho']]\n\ncombined = pd.concat([df1, df2, df3, df4], ignore_index=True)\ncombined = combined.drop_duplicates()\n\npairs = combined.apply(lambda row: f\"{row['zho']} = {row['pyu']}\", axis=1)\nreference_text = \"\\n\".join(pairs.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:36:03.449354Z","iopub.execute_input":"2025-06-11T17:36:03.449693Z","iopub.status.idle":"2025-06-11T17:36:03.798918Z","shell.execute_reply.started":"2025-06-11T17:36:03.449673Z","shell.execute_reply":"2025-06-11T17:36:03.798413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clean(text):\n    return re.sub(r\"[^\\w\\s]\", \"\", text).strip()\n\ndef fuzzy_search_entries(reference_text: str, keyword: str, top_k: int = 50) -> str:\n    keyword = clean(keyword)\n    chars = list(keyword)  # 拆成單個中文字\n\n    matches = []\n    for line in reference_text.split(\"\\n\"):\n        cleaned_line = clean(line)\n        if any(c in cleaned_line for c in chars):\n            matches.append(line)\n\n    return \"\\n\".join(matches[:top_k]) if matches else \"找不到相關翻譯資料。\"\n\n\nclass LLMAgent:\n    def __init__(self, role_description, task_description, references=None):\n        self.role_description = role_description\n        self.task_description = task_description\n        self.references = references\n\n    def inference(self, message: str, ref: str = None) -> str:\n        used_references = ref if ref is not None else self.references\n        messages = [\n            {\"role\": \"system\", \"content\": self.role_description},\n            {\"role\": \"user\", \"content\": used_references},\n            {\"role\": \"user\", \"content\": f\"{self.task_description}：「{message}」\"},\n        ]\n        return generate_response(llama3, messages)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:36:06.473385Z","iopub.execute_input":"2025-06-11T17:36:06.474046Z","iopub.status.idle":"2025-06-11T17:36:06.481638Z","shell.execute_reply.started":"2025-06-11T17:36:06.474010Z","shell.execute_reply":"2025-06-11T17:36:06.480827Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transtopyu_agent = LLMAgent(\n    role_description=\"你是聰明的語言模型，擅長翻譯繁體中文與卑南語，根據卑南語Agglutinative的特性，幫我參考以下資料後，利用特性嘗試翻譯文字至卑南語，並只保留卑南語的翻譯結果，不須說明。\",\n    task_description=\"翻譯以下文字為卑南語，並只保留翻譯內容\",\n)\n\ntranstozho_agent = LLMAgent(\n    role_description=\"你是聰明的語言模型，擅長翻譯繁體中文與卑南語，根據卑南語Agglutinative的特性，幫我參考以下資料後，利用特性嘗試翻譯文字至繁體中文，並只保留繁體中文的翻譯結果，不須說明。\",\n    task_description=\"翻譯以下文字為繁體中文，並只保留翻譯內容\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:36:09.491471Z","iopub.execute_input":"2025-06-11T17:36:09.492011Z","iopub.status.idle":"2025-06-11T17:36:09.495749Z","shell.execute_reply.started":"2025-06-11T17:36:09.491989Z","shell.execute_reply":"2025-06-11T17:36:09.494994Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PART Additional","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\n\nto_pyu = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/zh_to_pyu_test.csv', header=None)[0]\nto_zho = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/pyu_to_zh_test.csv', header=None)[0]\n\ndef translate_with_agent(texts, src_lang='zho'):\n    results = []\n    for text in tqdm(texts, desc=f\"Translating from {src_lang}\"):\n        ref = fuzzy_search_entries(reference_text, text)\n        try:\n            if src_lang == 'zho':\n                result = transtopyu_agent.inference(text, ref=ref)\n            else:\n                result = transtozho_agent.inference(text, ref=ref)\n        except Exception as e:\n            result = \"ERROR\"\n        cleaned_result = result.replace(\"\\n\", \"\").replace(\"\\r\", \"\").strip()\n        results.append(cleaned_result)\n    return results\n\ntranslated_pyu = translate_with_agent(to_pyu.tolist(), src_lang='zho')\ntranslated_zho = translate_with_agent(to_zho.tolist(), src_lang='pyu')\n\nfinal = pd.DataFrame({\n    \"ID\": range(1, len(translated_pyu) + len(translated_zho) + 1),\n    \"answer\": translated_pyu + translated_zho\n})\nfinal['answer'] = final['answer'].fillna('ERROR')\nfinal.to_csv(\"submission.csv\", index=False, encoding='utf-8')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:36:15.704422Z","iopub.execute_input":"2025-06-11T17:36:15.704685Z","iopub.status.idle":"2025-06-11T17:37:44.823934Z","shell.execute_reply.started":"2025-06-11T17:36:15.704664Z","shell.execute_reply":"2025-06-11T17:37:44.823371Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PART5","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nimport re\n\n# 初始化比較用的 Agent，只允許模型回覆 1 或 2\ncompare_agent = LLMAgent(\n    role_description=\"你是翻譯品質評估專家，請從以下兩個翻譯版本中選出較佳者，只回覆「一」或「二」，不要輸出其他文字或說明，以下為第一版本。\",\n    task_description=\"以下為第二版本。\"\n)\n\n# 從模型回傳中提取「1」或「2」\ndef compare_translations(version_a, version_b):\n    try:\n        result = compare_agent.inference(message=version_b, ref=version_a)\n        match = re.search(r\"(一|二)\", result)\n        if match:\n            return match.group(1)\n    except Exception:\n        pass\n    return \"ERROR\"\n\n# 載入兩個翻譯版本\ndf_a = pd.read_csv('/kaggle/input/puyuma-translation/submission (3).csv')\ndf_b = pd.read_csv('/kaggle/working/submission.csv')\n\n# 比對並挑選較佳翻譯\nfinal_answers = []\nfor i in tqdm(range(len(df_a))):\n    ans_a = df_a.loc[i, \"answer\"]\n    ans_b = df_b.loc[i, \"answer\"]\n    choice = compare_translations(ans_a, ans_b)\n    if choice == \"一\":\n        final_answers.append(ans_a)\n    elif choice == \"二\":\n        final_answers.append(ans_b)\n    else:\n        final_answers.append(\"ERROR\")\n\n# 匯出最終結果\nfinal_df = pd.DataFrame({\n    \"ID\": range(1, len(final_answers) + 1),\n    \"answer\": final_answers\n})\nfinal_df.to_csv(\"final_submission.csv\", index=False, encoding='utf-8')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:40:38.837132Z","iopub.execute_input":"2025-06-11T17:40:38.837866Z","iopub.status.idle":"2025-06-11T17:40:50.334628Z","shell.execute_reply.started":"2025-06-11T17:40:38.837834Z","shell.execute_reply":"2025-06-11T17:40:50.333985Z"}},"outputs":[],"execution_count":null}]}