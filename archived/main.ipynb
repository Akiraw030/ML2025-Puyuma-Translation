{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12145189,"sourceType":"datasetVersion","datasetId":7649223},{"sourceId":12146153,"sourceType":"datasetVersion","datasetId":7649884},{"sourceId":431918,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":352091,"modelId":373359}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:24:41.126726Z","iopub.execute_input":"2025-06-12T16:24:41.126906Z","iopub.status.idle":"2025-06-12T16:24:41.349222Z","shell.execute_reply.started":"2025-06-12T16:24:41.126889Z","shell.execute_reply":"2025-06-12T16:24:41.348492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Package download\n\n!pip install sentencepiece -q\n!pip install transformers -q\n!pip install datasets -q\n!pip install peft -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:26:32.360347Z","iopub.execute_input":"2025-06-12T16:26:32.360929Z","iopub.status.idle":"2025-06-12T16:28:13.918923Z","shell.execute_reply.started":"2025-06-12T16:26:32.360898Z","shell.execute_reply":"2025-06-12T16:28:13.918230Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Part1","metadata":{}},{"cell_type":"code","source":"# Nllb loading\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n# model_name = \"facebook/nllb-200-distilled-600M\"\nmodel_name = \"facebook/nllb-200-3.3B\" # Larger model\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\ntokenizer.src_lang = \"zho_Hant\"\ntokenizer.tgt_lang = \"tgl_Latn\"\n# zho_Hant for Chinese traditional\n# eng_Latn for English\n# tgl_Latn for Puyuma (Use existing language tag)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:06:21.982616Z","iopub.execute_input":"2025-06-10T12:06:21.982842Z","iopub.status.idle":"2025-06-10T12:07:14.33397Z","shell.execute_reply.started":"2025-06-10T12:06:21.982819Z","shell.execute_reply":"2025-06-10T12:07:14.333138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data into dataframes\n\nimport pandas as pd\n\nlexicon = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/lexicon_no_en.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nlexicon.columns = ['pyu', 'zho']\n\nlexicon_en = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/lexicon.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nlexicon_en.columns = ['pyu', 'eng', 'zho']\n\nsentences = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/sentences_no_en.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nsentences.columns = ['pyu', 'zho']\n\nsentences_en = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/sentences.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nsentences_en.columns = ['pyu', 'eng', 'zho']\n\n#lexicon.sample(5)\n#lexicon_en.sample(10)\n#sentences.sample(5)\n#sentences_en.sample(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T13:25:27.431218Z","iopub.execute_input":"2025-06-11T13:25:27.431776Z","iopub.status.idle":"2025-06-11T13:25:27.760134Z","shell.execute_reply.started":"2025-06-11T13:25:27.431751Z","shell.execute_reply":"2025-06-11T13:25:27.759275Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Testing the performances of original tokenization\n\nimport re\n\ndef word_tokenize(text):\n    \n    return re.findall('(\\w+|[^\\w\\s])', text)\n\ndef df_tokenize(df):\n    df['pyu_toks'] = df.pyu.apply(tokenizer.tokenize)\n    df['zho_toks'] = df.zho.apply(tokenizer.tokenize)\n    df['pyu_words'] = df.pyu.apply(word_tokenize)\n    df['zho_words'] = df.zho.apply(word_tokenize)\n    \n    return df\n\ndef cal_tokperword(df):\n\n    stats = df[['pyu_toks', 'zho_toks', 'pyu_words', 'zho_words']].map(len).describe()\n    print(stats.pyu_toks['mean'] / stats.pyu_words['mean'])\n    print(stats.zho_toks['mean'] / stats.zho_words['mean'])\n\n    return stats\n\ndef check_unk(df, column):\n\n    texts_with_unk = [\n        text for text in df[column]\n        if tokenizer.unk_token_id in tokenizer(text).input_ids\n    ]\n    print(len(texts_with_unk))\n\nlexicon = df_tokenize(lexicon)\nlexicon_en = df_tokenize(lexicon_en)\nsentences = df_tokenize(sentences)\nsentences_en = df_tokenize(sentences_en)\n\nprint(\"toks per word of lexicon:\")\nstats_lexicon = cal_tokperword(lexicon)\nprint(\"toks per word of lexicon_en:\")\nstats_lexicon = cal_tokperword(lexicon_en)\nprint(\"toks per word of sentences:\")\nstats_sentences = cal_tokperword(sentences)\nprint(\"toks per word of sentences_en:\")\nstats_sentences = cal_tokperword(sentences_en)\n\nprint(\"total unk in lexicon zho:\")\ncheck_unk(lexicon, \"zho\")\nprint(\"total unk in lexicon pyu:\")\ncheck_unk(lexicon, \"pyu\")\nprint(\"total unk in lexicon_en zho:\")\ncheck_unk(lexicon_en, \"zho\")\nprint(\"total unk in lexicon_en pyu:\")\ncheck_unk(lexicon_en, \"pyu\")\nprint(\"total unk in sentences zho:\")\ncheck_unk(sentences, \"zho\")\nprint(\"total unk in sentences pyu:\")\ncheck_unk(sentences, \"pyu\")\nprint(\"total unk in sentences_en zho:\")\ncheck_unk(sentences_en, \"zho\")\nprint(\"total unk in sentences pyu:\")\ncheck_unk(sentences_en, \"pyu\")\n\n#show datas\n#lexicon.sample(10)\n#sentences.sample(10)\n#stats_lexicon\n#stats_sentences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:07:14.426649Z","iopub.execute_input":"2025-06-10T12:07:14.426945Z","iopub.status.idle":"2025-06-10T12:07:22.933167Z","shell.execute_reply.started":"2025-06-10T12:07:14.42692Z","shell.execute_reply":"2025-06-10T12:07:22.932286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training tokenizer for missing tokens\n\nimport pandas as pd\nfrom tqdm.auto import tqdm\nimport re\nfrom collections import Counter\nimport sentencepiece as spm\nfrom datasets import load_dataset\n\nall_texts = lexicon['zho'].dropna().tolist() + sentences['zho'].dropna().tolist() + lexicon_en['zho'].dropna().tolist() + sentences_en['zho'].dropna().tolist() + lexicon['pyu'].dropna().tolist() + sentences['pyu'].dropna().tolist() + lexicon_en['pyu'].dropna().tolist() + sentences_en['pyu'].dropna().tolist()\n\nall_texts_file = 'all_texts_plain.txt'\nwith open(all_texts_file, 'w', encoding='utf-8') as f:\n    for text in all_texts:\n        print(text, file=f)\n\nrequired_chars = set()\n\nfor text in tqdm(all_texts):\n    for char in text:\n        tokens = tokenizer.tokenize(char)\n        if tokens == ['▁', '<unk>']:\n            required_chars.add(char)\n\nrequired_chars_str = \"\".join(sorted(list(required_chars)))\nprint(f\"需要強制包含的單字元: {required_chars_str}\")\n\nspm.SentencePieceTrainer.train(\n    input=all_texts_file,\n    model_prefix='spm_new',\n    vocab_size=5800,\n    character_coverage=1,\n    num_threads=16,\n    train_extremely_large_corpus=False,\n    add_dummy_prefix=False,\n    max_sentencepiece_length=128,\n    max_sentence_length=4192 * 4,\n    pad_id=0,\n    eos_id=1,\n    unk_id=2,\n    bos_id=-1,\n    required_chars=required_chars_str,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:07:22.934027Z","iopub.execute_input":"2025-06-10T12:07:22.934391Z","iopub.status.idle":"2025-06-10T12:07:31.042169Z","shell.execute_reply.started":"2025-06-10T12:07:22.934366Z","shell.execute_reply":"2025-06-10T12:07:31.041208Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add trained tokens to tokenizer and model\n\nfrom sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\nfrom transformers import NllbTokenizer\n\nmodel_name = 'facebook/nllb-200-3.3B'\ntokenizer_nllb = NllbTokenizer.from_pretrained(model_name)\n\nsp_trained = spm.SentencePieceProcessor(model_file='spm_new.model')\nadded_spm = sp_pb2_model.ModelProto()\nadded_spm.ParseFromString(sp_trained.serialized_model_proto())\nold_spm_nllb = sp_pb2_model.ModelProto()\nold_spm_nllb.ParseFromString(tokenizer_nllb.sp_model.serialized_model_proto())\n\nnllb_tokens_set = {p.piece for p in old_spm_nllb.pieces}\nprev_min_score = old_spm_nllb.pieces[-1].score\nfor p in added_spm.pieces:\n    piece = p.piece\n    if p.type != 1:\n        continue\n    if piece not in nllb_tokens_set:\n        new_p = sp_pb2_model.ModelProto().SentencePiece()\n        new_p.piece = piece\n        new_p.score = p.score + prev_min_score\n        old_spm_nllb.pieces.append(new_p)\n\nNEW_SPM_NAME = 'spm_nllb_extended_268k.model'\nwith open(NEW_SPM_NAME, 'wb') as f:\n    f.write(old_spm_nllb.SerializeToString())\n\ntokenizer = NllbTokenizer.from_pretrained(model_name, vocab_file='spm_new.model')\nprint(len(tokenizer_nllb), len(tokenizer))\nadded_vocab = set(tokenizer.get_vocab()).difference(set(tokenizer_nllb.get_vocab()))\n#print(added_vocab)(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:07:31.043323Z","iopub.execute_input":"2025-06-10T12:07:31.044077Z","iopub.status.idle":"2025-06-10T12:07:37.986497Z","shell.execute_reply.started":"2025-06-10T12:07:31.044034Z","shell.execute_reply":"2025-06-10T12:07:37.985867Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PART2","metadata":{}},{"cell_type":"code","source":"from transformers.optimization import Adafactor\nfrom transformers import get_constant_schedule_with_warmup\nmodel.cuda();\noptimizer = Adafactor(\n    [p for p in model.parameters() if p.requires_grad],\n    scale_parameter=False,\n    relative_step=False,\n    lr=1e-4,\n    clip_threshold=1.0,\n    weight_decay=5e-3,\n)\nscheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=2000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:07:37.987263Z","iopub.execute_input":"2025-06-10T12:07:37.98755Z","iopub.status.idle":"2025-06-10T12:07:39.283558Z","shell.execute_reply.started":"2025-06-10T12:07:37.987525Z","shell.execute_reply":"2025-06-10T12:07:39.282928Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nLANGS = [('zho', 'zho_Hant'), ('pyu', 'tgl_Latn')]\n\ndfs = [lexicon, sentences, lexicon_en, sentences_en]\ndf_train = pd.concat([df[['pyu', 'zho']] for df in dfs], ignore_index=True)\n\ndef get_batch_pairs(batch_size, data=df_train):\n    (l1, long1), (l2, long2) = random.sample(LANGS, 2)\n    xx, yy = [], []\n    for _ in range(batch_size):\n        item = data.iloc[random.randint(0, len(data)-1)]\n        xx.append(item[l1])\n        yy.append(item[l2])\n    return xx, yy, long1, long2\n\nprint(get_batch_pairs(1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:07:39.284303Z","iopub.execute_input":"2025-06-10T12:07:39.284556Z","iopub.status.idle":"2025-06-10T12:07:39.294548Z","shell.execute_reply.started":"2025-06-10T12:07:39.284529Z","shell.execute_reply":"2025-06-10T12:07:39.293791Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 12  # 32 already doesn't fit well to 15GB of GPU memory\nmax_length = 256  # token sequences will be truncated\ntraining_steps = 50000  # Usually, I set a large number of steps,\n# and then just interrupt the training manually\nlosses = []  # with this list, I do very simple tracking of average loss\nMODEL_SAVE_PATH = '/kaggle/working/nllb_extended'  # on my Google drive","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T12:07:39.296771Z","iopub.execute_input":"2025-06-10T12:07:39.296995Z","iopub.status.idle":"2025-06-10T12:07:39.318919Z","shell.execute_reply.started":"2025-06-10T12:07:39.296978Z","shell.execute_reply":"2025-06-10T12:07:39.318383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport torch\nimport numpy as np\nfrom tqdm.auto import tqdm, trange\n\ndef cleanup():\n    gc.collect()\n    torch.cuda.empty_cache()\n\nmodel.train()\nx, y, loss = None, None, None\ncleanup()\n\ntq = trange(len(losses), training_steps)\nfor i in tq:\n    xx, yy, lang1, lang2 = get_batch_pairs(batch_size)\n    try:\n        tokenizer.src_lang = lang1\n        x = tokenizer(xx, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n        tokenizer.src_lang = lang2\n        y = tokenizer(yy, return_tensors='pt', padding=True, truncation=True, max_length=max_length).to(model.device)\n        # -100 is a magic value ignored in the loss function\n        # because we don't want the model to learn to predict padding ids\n        y.input_ids[y.input_ids == tokenizer.pad_token_id] = -100\n\n        loss = model(**x, labels=y.input_ids).loss\n        loss.backward()\n        losses.append(loss.item())\n\n        optimizer.step()\n        optimizer.zero_grad(set_to_none=True)\n        scheduler.step()\n\n    except RuntimeError as e:  # usually, it is out-of-memory\n        optimizer.zero_grad(set_to_none=True)\n        x, y, loss = None, None, None\n        cleanup()\n        print('error', max(len(s) for s in xx + yy), e)\n        continue\n\n    if i % 2000 == 0:\n        # each 1000 steps, I report average loss at these steps\n        print(i, np.mean(losses[-1000:]))\n\n    if i % 2000 == 0 and i > 0:\n        model.save_pretrained(MODEL_SAVE_PATH)\n        tokenizer.save_pretrained(MODEL_SAVE_PATH)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-10T15:43:10.432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PART3","metadata":{}},{"cell_type":"code","source":"from transformers import NllbTokenizer, AutoModelForSeq2SeqLM\nmodel_dir = '/kaggle/input/nllb-extended/other/45000steps/1/results/nllb_extended'\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir, local_files_only=True).cuda()\ntokenizer = NllbTokenizer.from_pretrained(model_dir, vocab_file='/kaggle/input/nllb-extended/other/45000steps/1/results/spm_new.model')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:12:35.745002Z","iopub.execute_input":"2025-06-12T16:12:35.745756Z","iopub.status.idle":"2025-06-12T16:12:39.435749Z","shell.execute_reply.started":"2025-06-12T16:12:35.745722Z","shell.execute_reply":"2025-06-12T16:12:39.434694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def translate(\n    text, src_lang='zho_Hant', tgt_lang='tgl_Latn', \n    a=32, b=3, max_input_length=1024, num_beams=4, **kwargs\n):\n    \"\"\"Turn a text or a list of texts into a list of translations\"\"\"\n    tokenizer.src_lang = src_lang\n    tokenizer.tgt_lang = tgt_lang\n    inputs = tokenizer(\n        text, return_tensors='pt', padding=True, truncation=True, \n        max_length=max_input_length\n    )\n    model.eval() # turn off training mode\n    result = model.generate(\n        **inputs.to(model.device),\n        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n        num_beams=num_beams, **kwargs\n    )\n    return tokenizer.batch_decode(result, skip_special_tokens=True)\n\nt = '我也沒帶錢耶!'\nprint(translate(t, 'zho_Hant', 'tgl_Latn'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T09:29:14.5314Z","iopub.execute_input":"2025-06-11T09:29:14.5317Z","iopub.status.idle":"2025-06-11T09:29:14.689735Z","shell.execute_reply.started":"2025-06-11T09:29:14.531679Z","shell.execute_reply":"2025-06-11T09:29:14.689087Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PART Additional","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport re\n\nmodel_dir = \"/kaggle/input/nllb-extended/other/45000steps/1/results/nllb_extended\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir, local_files_only=True)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_dir, local_files_only=True).to(\"cuda\")\n\ndef clean_output(text):\n    text = text.strip()\n    text = re.sub(r'^[\\\"“”「」『』、,.;:?!]+', '', text)  # 移除開頭標點\n    text = re.sub(r'[\\\"“”「」『』、,.;:?!]+$', '', text)  # 移除結尾標點\n    return text if text != \"\" else \"ERROR\"\n\ndef translate(\n    text, src_lang='zho_Hant', tgt_lang='tgl_Latn', \n    a=32, b=3, max_input_length=1024, num_beams=4, **kwargs\n):\n    tokenizer.src_lang = src_lang\n    tokenizer.tgt_lang = tgt_lang\n    inputs = tokenizer(\n        text, return_tensors='pt', padding=True, truncation=True, \n        max_length=max_input_length\n    ).to(model.device)\n\n    model.eval()\n    result = model.generate(\n        **inputs,\n        forced_bos_token_id=tokenizer.convert_tokens_to_ids(tgt_lang),\n        max_new_tokens=int(a + b * inputs.input_ids.shape[1]),\n        num_beams=num_beams, **kwargs\n    )\n    decoded = tokenizer.batch_decode(result, skip_special_tokens=True)\n    decoded = [clean_output(t) for t in decoded]\n    return decoded\n\nto_pyu = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/zh_to_pyu_test.csv', header=None)[0]\nto_zho = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/pyu_to_zh_test.csv', header=None)[0]\n\ntranslated_pyu = translate(to_pyu.tolist(), src_lang='zho_Hant', tgt_lang='pyu_Latn')\ntranslated_zho = translate(to_zho.tolist(), src_lang='pyu_Latn', tgt_lang='zho_Hant')\n\nfinal = pd.DataFrame({\n    \"ID\": range(1, len(translated_pyu) + len(translated_zho) + 1),\n    \"answer\": translated_pyu + translated_zho\n})\nfinal['answer'] = final['answer'].fillna('ERROR')\nfinal.to_csv(\"submission.csv\", index=False, encoding='utf-8')\nfinal.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:29:06.830818Z","iopub.execute_input":"2025-06-12T16:29:06.831100Z","iopub.status.idle":"2025-06-12T16:29:31.414572Z","shell.execute_reply.started":"2025-06-12T16:29:06.831077Z","shell.execute_reply":"2025-06-12T16:29:31.413518Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## PART4","metadata":{}},{"cell_type":"code","source":"!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122 -q\n!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean -q\n\n!wget https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K.gguf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:31:24.702394Z","iopub.execute_input":"2025-06-12T16:31:24.702996Z","iopub.status.idle":"2025-06-12T16:32:03.638897Z","shell.execute_reply.started":"2025-06-12T16:31:24.702973Z","shell.execute_reply":"2025-06-12T16:32:03.638127Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m120.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsigstore 3.6.1 requires rich~=13.0, but you have rich 14.0.0 which is incompatible.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.5, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-spark-connect 0.5.2 requires google-api-core>=2.19.1, but you have google-api-core 1.34.1 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngoogle-genai 0.8.0 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\ngoogle-cloud-bigtable 2.28.1 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m--2025-06-12 16:31:53--  https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q6_K.gguf\nResolving huggingface.co (huggingface.co)... 3.165.160.12, 3.165.160.11, 3.165.160.59, ...\nConnecting to huggingface.co (huggingface.co)|3.165.160.12|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cas-bridge.xethub.hf.co/xet-bridge-us/66f457f561dbc064f4a6413f/da1f5f07d4a337c67bfa4bc7ed0a241a4fe77883ba5924357b8c2e1db5ec797d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250612%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250612T160248Z&X-Amz-Expires=3600&X-Amz-Signature=9924d63b27c2674337e25d4648acb7f60c87765aac241e0cf2d0f6168631aca9&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-3B-Instruct-Q6_K.gguf%3B+filename%3D%22Llama-3.2-3B-Instruct-Q6_K.gguf%22%3B&x-id=GetObject&Expires=1749747768&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0OTc0Nzc2OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NmY0NTdmNTYxZGJjMDY0ZjRhNjQxM2YvZGExZjVmMDdkNGEzMzdjNjdiZmE0YmM3ZWQwYTI0MWE0ZmU3Nzg4M2JhNTkyNDM1N2I4YzJlMWRiNWVjNzk3ZCoifV19&Signature=ao8fF846PBH9-dKEWbSeO8o2mK834dbnvIr8XZ5H5VvRA0JT-H3IRl9qpKWAjbiGTTeaYYIEHy4F9l3N%7ElAvez2Q6B9Ed9Mj4O61pYVfLura5GJbBF953EkoluWL-aGGd8U-82ewECVpajL64sPZYIJQBAcsMtbdxuXQTwwZgcLZcDZrfNGJOWa155PvT5QdyjPxlfFfro3GEu-OUA%7EtH-%7EjuEJa2vIuKZ1REsyX2l75kqSVB%7ELDPSsOkqRT8JqV81PeqsTeFMn4BtCGKmAiCzumeFPBlGomGh2nTOrIzpQSgPVxPgE8LH-wwx0SSHLKlBbDTtzcyS1B3DZ0jco%7EpA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n--2025-06-12 16:31:53--  https://cas-bridge.xethub.hf.co/xet-bridge-us/66f457f561dbc064f4a6413f/da1f5f07d4a337c67bfa4bc7ed0a241a4fe77883ba5924357b8c2e1db5ec797d?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250612%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250612T160248Z&X-Amz-Expires=3600&X-Amz-Signature=9924d63b27c2674337e25d4648acb7f60c87765aac241e0cf2d0f6168631aca9&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-3B-Instruct-Q6_K.gguf%3B+filename%3D%22Llama-3.2-3B-Instruct-Q6_K.gguf%22%3B&x-id=GetObject&Expires=1749747768&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0OTc0Nzc2OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NmY0NTdmNTYxZGJjMDY0ZjRhNjQxM2YvZGExZjVmMDdkNGEzMzdjNjdiZmE0YmM3ZWQwYTI0MWE0ZmU3Nzg4M2JhNTkyNDM1N2I4YzJlMWRiNWVjNzk3ZCoifV19&Signature=ao8fF846PBH9-dKEWbSeO8o2mK834dbnvIr8XZ5H5VvRA0JT-H3IRl9qpKWAjbiGTTeaYYIEHy4F9l3N%7ElAvez2Q6B9Ed9Mj4O61pYVfLura5GJbBF953EkoluWL-aGGd8U-82ewECVpajL64sPZYIJQBAcsMtbdxuXQTwwZgcLZcDZrfNGJOWa155PvT5QdyjPxlfFfro3GEu-OUA%7EtH-%7EjuEJa2vIuKZ1REsyX2l75kqSVB%7ELDPSsOkqRT8JqV81PeqsTeFMn4BtCGKmAiCzumeFPBlGomGh2nTOrIzpQSgPVxPgE8LH-wwx0SSHLKlBbDTtzcyS1B3DZ0jco%7EpA__&Key-Pair-Id=K2L8F4GPSG1IFC\nResolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.217.88, 18.238.217.126, 18.238.217.63, ...\nConnecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.217.88|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2643853856 (2.5G)\nSaving to: ‘Llama-3.2-3B-Instruct-Q6_K.gguf’\n\nLlama-3.2-3B-Instru 100%[===================>]   2.46G   264MB/s    in 9.7s    \n\n2025-06-12 16:32:03 (261 MB/s) - ‘Llama-3.2-3B-Instruct-Q6_K.gguf’ saved [2643853856/2643853856]\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from llama_cpp import Llama\n\n# Load the model onto GPU\nllama3 = Llama(\n    \"/kaggle/working/Llama-3.2-3B-Instruct-Q6_K.gguf\",\n    verbose=False,\n    n_gpu_layers=-1,\n    n_ctx=30000,\n)\n\ndef generate_response(_model: Llama, _messages: str) -> str:\n    \n    _output = _model.create_chat_completion(\n        _messages,\n        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n        max_tokens=512,\n        temperature=0.1,\n        repeat_penalty=2.0,\n    )[\"choices\"][0][\"message\"][\"content\"]\n    return _output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T16:35:09.632752Z","iopub.execute_input":"2025-06-12T16:35:09.633422Z","iopub.status.idle":"2025-06-12T16:35:11.692299Z","shell.execute_reply.started":"2025-06-12T16:35:09.633396Z","shell.execute_reply":"2025-06-12T16:35:11.691756Z"}},"outputs":[{"name":"stderr","text":"llama_new_context_with_model: n_ctx_per_seq (30016) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport asyncio\n\n# === 資料載入 ===\nlexicon = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/lexicon_no_en.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nlexicon.columns = ['pyu', 'zho']\n\nlexicon_en = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/lexicon.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nlexicon_en.columns = ['pyu', 'eng', 'zho']\n\nsentences = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/sentences_no_en.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nsentences.columns = ['pyu', 'zho']\n\nsentences_en = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/sentences.csv', sep=\",\", quotechar='\"', header=None, encoding=\"utf-8\")\nsentences_en.columns = ['pyu', 'eng', 'zho']\n\n# === 組合與清洗資料 ===\ndf1 = lexicon[['pyu', 'zho']]\ndf2 = lexicon_en[['pyu', 'zho']]\ndf3 = sentences[['pyu', 'zho']]\ndf4 = sentences_en[['pyu', 'zho']]\n\ncombined = pd.concat([df1, df2, df3, df4], ignore_index=True)\ncombined = combined.drop_duplicates()\n\npairs = combined.apply(lambda row: f\"{row['zho']} = {row['pyu']}\", axis=1)\nreference_text = \"\\n\".join(pairs.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T17:15:05.006102Z","iopub.execute_input":"2025-06-12T17:15:05.006663Z","iopub.status.idle":"2025-06-12T17:15:05.062125Z","shell.execute_reply.started":"2025-06-12T17:15:05.006638Z","shell.execute_reply":"2025-06-12T17:15:05.061414Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"grammar_book = \"第三章 詞彙與構詞本章主要討論知本卑南語的詞彙結構及主要構詞方式。3.1 主要構詞單位3.1.1 詞及詞素詞 (Word)：是句子結構中的最小單位 。 音節多寡：詞可以是單音節（如 mu「當...的時候」）、雙音節（如 ru.ma「房子」、pa.kan「餵」）、三音節（如 mu.di.ngan「臉」）或四音節以上（如 pu.a.li.ma「戴戒指」） 。組成成份：有些詞由單一成分組成，無法再分解，稱為單純詞（如 ru.ma「房子」） 。有些則由兩個（如 pa+kan「餵(使...吃)」）或更多成分組成，稱為複雜詞 。語意與句法功能：具有實質語意的稱為實詞（如名詞 pu.ran「檳榔」、動詞 pakan「餵」），屬於開放性詞類 。具有句法功能的稱為虛詞或功能詞（如格位標記 za「斜格」、代名詞 inku「我,主格」），屬於封閉性詞類 。詞素 (Morpheme)：是語言系統中具有意義或語法功能的最小單位 。 自由詞素：可以獨立存在的詞素，如 velrvelr「香蕉」、kayan「坐」、inku「我、主格」 。附著詞素：一定要附加在某個詞上，不能單獨使用的詞素 。可分為詞綴（如 ki-「取得」、pa-「使...」）和依附詞（如代名詞 =ku「我 主格」） 。3.1.2 詞根及詞幹詞根 (Root)：是最小且具有意義的詞素，不包括任何附加成份（如重疊或詞綴） 。例如 matra「眼睛」是詞根，因為無法再切割成有意義的 *ma- 或 *-tra 。詞根不分長短，例如 velrvelr「香蕉」也是一個詞根 。詞幹 (Stem)：可以單純由一個詞根構成，也可以包含詞根再加上詞綴 。例如，在 pa-nadam「教」中，詞幹是 nadam「學習」；而在 ki-pa-nadam「受教」中，詞幹則是 pa-nadam 。3.1.3 詞綴及依附詞詞綴與依附詞都不能單獨使用。在本書中，詞綴用連字符號 - 標示，依附詞則用等號 = 標示 。例如：在 tu veray-ay=ku「他給我」一句中，-ay 是詞綴，=ku 是依附詞 。臺灣南島語在加詞綴的過程中，通常會影響重音。例如 inavă「好」加上後綴 -an 後，重音會移至最後音節，變成 inava-án 。詞綴可分為兩類：屈折詞綴：附加在特定詞類上，用來表示語法功能（如語氣、時貌），但不改變該詞的詞類。例如：動詞 pukpuk「打」→ pukpuk-u!「打！」（命令式動詞） 。衍生詞綴：會產生不同的語意並（或）造成詞類的改變。例如：名詞 avay「年糕」→ 動詞 tu-avay「做年糕」；動詞 ekan「吃」→ 名詞 a-ekan-an「食物」 。依附詞與詞綴的不同在於，依附詞不選擇其「寄主詞」的詞類或語意，通常依附於句中第一個成分。例如，依附詞 =ku「我」可以依附於動詞、名詞或否定詞 。 mapungaw=ku.（我頭暈）vs. a sinsi=ku.（我是老師） 'azi=ku mapungaw.（我沒頭暈）vs. melri=ku a sinsi.（我不是老師） 3.1.4 同位詞 (Allomorph)同位詞是一個詞素在不同語音環境下的變體 。主事焦點中綴 <em> 有三個同位詞：m-、me- 及 <en> 。 m-：出現在母音開頭的動詞上，如 m-abak「裝」、m-alak「拿」 。me-：出現在 n 及 ng 開頭的動詞上，如 me-na'u「看」、me-ngara「等」 。<en>：出現在 v 及 p 開頭的動詞上，如 v<en>usus「騙」、p<en>a'ing「打噴嚏」 。<em>：出現在其餘語音環境，如 k<em>ayan「坐下」、tr<em>evel「理髮」 。表完成的中綴 <in> 有兩個同位詞：in- 及 ni- 。 in-：出現在母音開頭的動詞上，如 in-abak「被裝了的」、in-alak「被拿了的」 。ni-：出現在 n 及 ng 開頭的動詞上，如 ni-na'u「被看了的」、ni-ngara「被等了的」 。<in>：出現在其餘語音環境，如 v<in>usus「被騙了的」、k<in>ayan「被坐下」 。3.2 構詞方法知本卑南語的主要構詞方法包括加綴、重疊及複合 。3.2.1 加綴 (Affixation)前綴：改變詞類（名詞 → 動詞） ki-：「取得」，如 ki-paisu「要錢」 。mi-：「穿、戴、帶、有」，如 mi-kavang「穿衣」、mi-paisu「有錢」 。mutu-：「變成」，如 mutu-trau「變成人」 。tara-：「使用」，如 tara-puyuma「說卑南語」 。tu-：「製造、產生」，如 tua-avay「做糯米糕」 。tinu-：「模擬」，如 tinu-maizang「實習長老」 。不改變詞類 mare-（名詞→名詞）：「互相」，如 mare-wadi「兄弟姊妹」 。kara-（動詞→動詞）：「一起」，如 kara-kayan「坐在一起」 。pa-（動詞→動詞）：「使、讓」，如 pa-ekan「餵、使吃」 。mara-（動詞→動詞）：「比較」，如 mara-lriketri「較短」 。中綴 ： <in>：表示「完成」，如 d<in>away「做好的」 。<em>：表示「主事焦點」，如 k<em>ayan「坐」 。後綴 ： -an：將動詞轉為名詞，表「地方」，如 takesi-an「學校」、tra'i-tra'i-an「廁所」 。環綴 ： ka-...-an：表示「做...的時期」或「真正的...」，如 ka-salem-an「種植的季節」、ka-ruma-an「主屋」 。<in>...anan：表示「...的成員」，如 z<in>pekalr-anan「村民」 。3.2.2 重疊 (Reduplication)Ca-重疊：重疊詞根倒數第二音節的輔音再加上母音 /a/ 。 在數詞上表達「數人」：如 zuwa「二」 → za-zuwa「兩人」 。在動詞上表示「進行」或「非實現」：如 senay「唱」 → s<em>a-senay「正在唱」 。表示「互相」：如 karatr「咬」 → ma-ka-karatr「互咬」 。在名詞上表示「通稱」或「多數」：如 trau「人」 → tra-trau-an「人類」 。形成表示「處所」的名詞：如 dirus「洗澡」 → da-dirus-an「洗澡間」 。形成表示「工具」的名詞：如 ngisil「刷」 → nga-ngisil「牙刷」 。雙音節重疊：重疊字根倒數兩個音節 。 加在名詞表示「複數」或「總稱」：如 zenan → zena-zenan「山脈」、tralun「草」→ tralu-tralun「草叢」 。加在動態動詞表「動作重複」：如 me-na'u「看」 → me-nau-na'u「不斷地看」 。加在靜態動詞「加重程度」：如 dawilr「遠」 → dawidawilr「很遠」 。複雜重疊：結合兩種以上方式的重疊 。例如 wari「天」 → wa-wari-wari「每天」 。3.3 擬聲詞擬聲詞是用聲音摹仿事物、動作或自然界聲音的詞彙 。動物：ngiaw「貓」、up'up「牛蛙」、maymay「鴨」、wa wa「烏鴉」、tutur「鴿子」 。昆蟲：tengteng「蜻蜓」、kengkeng「蚊子」 。動詞 (模擬動作聲音)：tiktik「雕刻聲」、tuktuk「鐵鎚聲」、taktak「砍樹聲」、pukpuk「用棍子打孩子聲」 。動詞 (模擬自然界聲音)：zerung「打雷聲」、treli「閃電」 。3.4 借詞知本卑南語的借詞來源有日語、台語及中文 。日語借詞：kupu「杯子」、layta「打火機」、sulippa「拖鞋」、iga「電影」、kikay「機器」、sinsi「老師」、hikoki「飛機」、dingwa「電話」、tuki「時鐘/手錶」、wasabi「芥末」、tomato「番茄」、sibiru「西裝」 。台語借詞：dolayba「螺ising起子」、tangsuy「雨衣」、ising「醫生」、voksi「牧師」、tu「桌子」、tawyu「醬油」、pisay「白菜」、kiw「茄子」 。3.5 詞類類別詞類可分為成員數量有限的封閉性詞類（如代名詞、副詞）和成員沒有限制的開放性詞類（如動詞、名詞） 。3.5.1 開放性詞類動詞和名詞的區分：從構詞上區分不易，句法上的證據比較可靠 。 指示代名詞可以出現在名詞前（ini na alrak「這個孩子」），但不能出現在動詞前 。自由式的代名詞可以出現在名詞前，但不能在動詞前。例如在 tu ngarayaw tu sinsi「他等他的老師」中，tu sinsi 可以被 nantu sinsi 取代，但 tu ngarayaw 不能被取代 。名詞用 melri 來否定（melri a sinsi intaw.「他不是老師」），動詞用 'azi 來否定（'azi maekan za vulraw.「他不吃魚」） 。名詞：可分為三類，各由不同格位標記來標示 。 人稱專有名詞：包含人名及親屬稱謂，有單複數之分。如 zua i tainataw.「他的媽媽來了。」 。處所名詞：如 adawilr i Tayhok.「台北很遠。」 。一般名詞：有「限定」與「非限定」之分。如 ulra a trau i ruma.「房子裡有人。」 。動詞：動詞上的焦點詞綴決定了主語的語意角色，主要有四種焦點：主事者 (<em>)、受事者 (-aw)、處所 (-ay)、受惠者/工具 (-anay) 。知本卑南語沒有獨立的「形容詞」詞類，其功能由靜態動詞（如「喜歡」、「害怕」）承擔 。動詞分為動態動詞和靜態動詞。動態動詞通常帶 <em>（或其同位詞），而靜態動詞帶 ma- 。兩者在命令句、否定句、非實現貌及使役句中有不同的標記方式 。3.5.2 封閉性詞類格位標記：出現在名詞或名詞組之前，標示其語意角色或文法關係 。人稱代名詞：指「我」、「你」、「他」等。第一人稱複數常區分「包含式」（咱們）和「排除式」（我們） 。指定代名詞：即指示代名詞，可單獨使用或修飾名詞，形式可能因與說話者距離、是否可見、單複數等因素而異 。疑問詞：用於構成特殊問句，如「誰」、「什麼」、「何處」等 。數字：分為基數詞與序數詞等 。詞組標記和子句標記：詞組標記：如連繫詞 na，常出現在名詞之間。例：zua na tatelru na trau.「那三個人來了。」 。並列連詞如 zi「和」。例：vi'as na kadaw zi, pitalupung...「太陽熱，而且要戴帽子...」 。子句標記：如主題標記 mu 和從屬連詞 nu「當」。例：na vavuy mu, tu kuwangaw ni ama za kuwang.「（那隻）山豬，爸爸用槍射了。」 。感嘆詞：表示驚訝、痛苦、悲傷等感情，如 iwa「唉呀」 。\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport jieba\n\ndef clean(text):\n    return re.sub(r\"[^\\w\\s]\", \"\", text).strip()\n    \ndef is_valid_word(word: str, min_alpha: int = 4) -> bool:\n    num_alpha = sum(c.isalpha() for c in word)\n    return num_alpha >= min_alpha\n\ndef fuzzy_search_entries(reference_text: str, keyword: str, top_k: int = 50) -> str:\n    keyword = clean(keyword)\n\n    ignore_words = {\"的\", \"一\", \"在\", \" \"}\n    raw_words = jieba.cut(keyword)\n\n    words = []\n    for w in raw_words:\n        if w in ignore_words:\n            continue\n        if re.fullmatch(r'[a-zA-Z\\']+', w):\n            if not is_valid_word(w):\n                continue\n        words.append(w)\n\n    print(\"有效詞彙：\", words)\n\n    if not words:\n        return \"無可參考條目\"\n\n    matches = []\n    for line in reference_text.split(\"\\n\"):\n        cleaned_line = clean(line)\n        if any(w in cleaned_line for w in words):\n            matches.append(line)\n\n    return \"\\n\".join(matches[:top_k]) if matches else \"找不到相關翻譯資料。\"\n\nclass LLMAgent:\n    def __init__(self, role_description, task_description, references=None):\n        self.role_description = role_description\n        self.task_description = task_description\n        self.references = references\n\n    def inference(self, message: str, ref: str = None, nllb: str = None) -> str:\n        used_references = ref if ref is not None else self.references\n        messages = [\n            {\"role\": \"system\", \"content\": self.role_description},\n            {\"role\": \"user\", \"content\": f\"以下是卑南語的文法說明，可作為翻譯參考：「{grammar_book}」\"},\n            {\"role\": \"user\", \"content\": f\"以下是相關詞彙的翻譯，可作為翻譯參考：「{used_references}」\"},\n            {\"role\": \"user\", \"content\": f\"以下是NLLB模型輸出的翻譯結果，可作為翻譯參考：「{nllb}」\"},\n            {\"role\": \"user\", \"content\": f\"{self.task_description}：「{message}」\"},\n        ]\n        return generate_response(llama3, messages)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T17:46:34.265262Z","iopub.execute_input":"2025-06-12T17:46:34.265758Z","iopub.status.idle":"2025-06-12T17:46:34.274257Z","shell.execute_reply.started":"2025-06-12T17:46:34.265734Z","shell.execute_reply":"2025-06-12T17:46:34.273502Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"transtopyu_agent = LLMAgent(\n    role_description=\"你是聰明的語言模型，擅長翻譯繁體中文與卑南語，幫我參考以下資料後，利用特性嘗試翻譯文字至卑南語，並只保留卑南語的翻譯結果，不須說明。\",\n    task_description=\"翻譯以下文字為卑南語，並只保留翻譯內容\",\n)\n\ntranstozho_agent = LLMAgent(\n    role_description=\"你是聰明的語言模型，擅長翻譯繁體中文與卑南語，幫我參考以下資料後，利用特性嘗試翻譯文字至繁體中文，並只保留繁體中文的翻譯結果，不須說明。\",\n    task_description=\"翻譯以下文字為繁體中文，並只保留翻譯內容\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T17:46:36.349278Z","iopub.execute_input":"2025-06-12T17:46:36.349829Z","iopub.status.idle":"2025-06-12T17:46:36.353522Z","shell.execute_reply.started":"2025-06-12T17:46:36.349807Z","shell.execute_reply":"2025-06-12T17:46:36.352758Z"}},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":"## PART Additional","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\n\nto_pyu = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/zh_to_pyu_test.csv', header=None)[0]\nto_zho = pd.read_csv('/kaggle/input/ml2025-bonus/dataset/pyu_to_zh_test.csv', header=None)[0]\nnllb_df = pd.read_csv(\"/kaggle/input/nllboutput/submission (3).csv\")\nnllb_outputs = nllb_df['answer'].tolist()\ngrammar_book = \n\ndef translate_with_agent(texts, src_lang='zho'):\n    results = []\n    for i, text in enumerate(tqdm(texts, desc=f\"Translating from {src_lang}\")):\n        ref = fuzzy_search_entries(reference_text, text)\n        try:\n            nllb_result = nllb_outputs[i] if i < len(nllb_outputs) else \"\"\n            print(nllb_result)\n            if src_lang == 'zho':\n                result = transtopyu_agent.inference(text, ref=ref, nllb=nllb_result)\n            else:\n                result = transtozho_agent.inference(text, ref=ref, nllb=nllb_result)\n        except Exception as e:\n            result = \"ERROR\"\n        cleaned_result = result.replace(\"\\n\", \"\").replace(\"\\r\", \"\").strip()\n        results.append(cleaned_result)\n    return results\n\ntranslated_pyu = translate_with_agent(to_pyu.tolist(), src_lang='zho')\ntranslated_zho = translate_with_agent(to_zho.tolist(), src_lang='pyu')\n\nfinal = pd.DataFrame({\n    \"ID\": range(1, len(translated_pyu) + len(translated_zho) + 1),\n    \"answer\": translated_pyu + translated_zho\n})\nfinal['answer'] = final['answer'].fillna('ERROR')\nfinal.to_csv(\"submission.csv\", index=False, encoding='utf-8')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-12T17:47:11.585389Z","iopub.execute_input":"2025-06-12T17:47:11.585876Z","iopub.status.idle":"2025-06-12T17:47:15.328908Z","shell.execute_reply.started":"2025-06-12T17:47:11.585851Z","shell.execute_reply":"2025-06-12T17:47:15.327829Z"}},"outputs":[{"name":"stderr","text":"Translating from zho:   0%|          | 0/60 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"有效詞彙： ['現在', '初鹿', '部落', '祭司', '是', '由長', '老', '推舉', '選任']\naaydan ziya na rahan naulid i zekalr zi natrepa kana valalisen,\n","output_type":"stream"},{"name":"stderr","text":"Translating from zho:   2%|▏         | 1/60 [00:01<01:04,  1.09s/it]","output_type":"stream"},{"name":"stdout","text":"有效詞彙： ['你', '今天', '穿', '得', '好', '漂亮衣服', '上', '有', '好多', '鈴鐺']\na treme'utran mu, mutrungutrungulr kana 'azi marekamelri za kavang.\n","output_type":"stream"},{"name":"stderr","text":"Translating from zho:   3%|▎         | 2/60 [00:01<00:47,  1.21it/s]","output_type":"stream"},{"name":"stdout","text":"有效詞彙： ['搖擺', '竹蔭下', '得', '永眠', '之', '所']\na menadanadam muwaarak kanizu na suwan mu, ulra na suwan mu,\n","output_type":"stream"},{"name":"stderr","text":"Translating from zho:   5%|▌         | 3/60 [00:03<01:07,  1.18s/it]","output_type":"stream"},{"name":"stdout","text":"有效詞彙： ['在家', '準備', '男人', '衣物', '到', '凱旋門', '及', '集會', '所', '佈', '置']\na puresaring, ki物'urian到valenan, kitrepa kana matengetengez,\n","output_type":"stream"},{"name":"stderr","text":"Translating from zho:   5%|▌         | 3/60 [00:03<01:10,  1.23s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2684420073.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mtranslated_pyu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_with_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_pyu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zho'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mtranslated_zho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate_with_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_zho\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pyu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/2684420073.py\u001b[0m in \u001b[0;36mtranslate_with_agent\u001b[0;34m(texts, src_lang)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnllb_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msrc_lang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'zho'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranstopyu_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnllb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnllb_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranstozho_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnllb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnllb_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/1424422085.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, message, ref, nllb)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{self.task_description}：「{message}」\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         ]\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllama3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_31/1292466462.py\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(_model, _messages)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_messages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     _output = _model.create_chat_completion(\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0m_messages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<|eot_id|>\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"<|end_of_text|>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mcreate_chat_completion\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[0m\n\u001b[1;32m   1996\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mllama_chat_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chat_completion_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m         )\n\u001b[0;32m-> 1998\u001b[0;31m         return handler(\n\u001b[0m\u001b[1;32m   1999\u001b[0m             \u001b[0mllama\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp/llama_chat_format.py\u001b[0m in \u001b[0;36mchat_completion_handler\u001b[0;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m                 )\n\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         completion_or_chunks = llama.create_completion(\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mcreate_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1830\u001b[0m             \u001b[0mchunks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCreateCompletionStreamResponse\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompletion_or_chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1831\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1832\u001b[0;31m         \u001b[0mcompletion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompletion_or_chunks\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1833\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcompletion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36m_create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1315\u001b[0m         \u001b[0mfinish_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"length\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         \u001b[0mmultibyte_fix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m         for token in self.generate(\n\u001b[0m\u001b[1;32m   1318\u001b[0m             \u001b[0mprompt_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;31m# Eval and sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msample_idx\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                 token = self.sample(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp/llama.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_past\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_past\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m             )\n\u001b[0;32m--> 643\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0;31m# Save tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn_past\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mn_past\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_tokens\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/llama_cpp/_internals.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLlamaBatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         return_code = llama_cpp.llama_decode(\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":79},{"cell_type":"markdown","source":"## PART5","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nimport re\n\n# 初始化比較用的 Agent，只允許模型回覆 1 或 2\ncompare_agent = LLMAgent(\n    role_description=\"你是翻譯品質評估專家，請從以下兩個翻譯版本中選出較佳者，只回覆「一」或「二」，不要輸出其他文字或說明，以下為第一版本。\",\n    task_description=\"以下為第二版本。\"\n)\n\n# 從模型回傳中提取「1」或「2」\ndef compare_translations(version_a, version_b):\n    try:\n        result = compare_agent.inference(message=version_b, ref=version_a)\n        match = re.search(r\"(一|二)\", result)\n        if match:\n            return match.group(1)\n    except Exception:\n        pass\n    return \"ERROR\"\n\n# 載入兩個翻譯版本\ndf_a = pd.read_csv('/kaggle/input/puyuma-translation/submission (3).csv')\ndf_b = pd.read_csv('/kaggle/working/submission.csv')\n\n# 比對並挑選較佳翻譯\nfinal_answers = []\nfor i in tqdm(range(len(df_a))):\n    ans_a = df_a.loc[i, \"answer\"]\n    ans_b = df_b.loc[i, \"answer\"]\n    choice = compare_translations(ans_a, ans_b)\n    if choice == \"一\":\n        final_answers.append(ans_a)\n    elif choice == \"二\":\n        final_answers.append(ans_b)\n    else:\n        final_answers.append(\"ERROR\")\n\n# 匯出最終結果\nfinal_df = pd.DataFrame({\n    \"ID\": range(1, len(final_answers) + 1),\n    \"answer\": final_answers\n})\nfinal_df.to_csv(\"final_submission.csv\", index=False, encoding='utf-8')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-11T17:40:38.837132Z","iopub.execute_input":"2025-06-11T17:40:38.837866Z","iopub.status.idle":"2025-06-11T17:40:50.334628Z","shell.execute_reply.started":"2025-06-11T17:40:38.837834Z","shell.execute_reply":"2025-06-11T17:40:50.333985Z"}},"outputs":[],"execution_count":null}]}